# Пример скрапера для сайта cnews.ru

Скрапер ориентирован на обработку XML-карты сайта. Сценарий подразумевает три этапа — загрузка метаданных, заполнение очереди ссылок и скачивание HTML.

### Структура
- `config.py` — параметры: адрес сайта, папка данных, настройки базы sqlite и задержка между запросами.
- `get_meta.py` — получает `robots.txt`, находит ссылку на sitemap и скачивает его в `data/`. Также выводит количество ссылок и примерную информацию.
- `make_pages_queue.py` — парсит скачанную карту, сохраняет уникальные `loc` в таблицу `Url` и помечает их состоянием `new`.
- `download_pages.py` — извлекает все значения `state = 'new'`, скачивает HTML через `httpx`, сохраняет содержимое с хешем и временем в `pages_content`, переводит строки таблицы `Url` в `downloaded`.

### Подготовка
1. Создайте виртуальное окружение и установите зависимости:
   ```
   python -m venv .venv
   .venv/bin/pip install -r requirements.txt
   ```
2. Убедитесь, что существует папка `data/`. Именно туда скрипты создают `robots.txt`, `sitemap.xml` и базу `database.sqlite`.
3. При необходимости скорректируйте `config.py` — например, увеличьте `DELAY`, если сайт начинает отказываться.

### Использование
1. `python get_meta.py` — скачивает `robots.txt`, определяет sitemap и сохраняет его. После завершения внутри `data/` появится `robots.txt`, `sitemap.xml`.
2. `python make_pages_queue.py` — парсит `data/sitemap.xml`, записывает ссылки в БД SQLite, в таблицу `Url`.
3. `python download_pages.py` — проходит по таблице `Url`, скачивает HTML, сохраняет их в отдельную таблицу в этой же БД, обновляет статус каждой записи.

### Состояние базы
-- `Url` — `id`, `url`, `type`, `state`, `note`. `state` принимает `new`, `downloaded`.
- `pages_content` — `id`, `content`, `url`, `content_hash`, `download_dt`. `content_hash` вычисляется через `blake2s`, `download_dt` хранит ISO-метку времени.
Проверить статистику можно командой:
```sh
sqlite3 data/database.sqlite "SELECT state, count(*) FROM Url GROUP BY state;"
```

### Расширения
- Подключите новый парсер нужного раздела — ищите ссылки из `pages_content` по ключевым блокам, сохраняйте в другой таблице.
- Используйте `httpx.AsyncClient`, если нужно массово параллельное скачивание (учитывайте `robots.txt`).


# TODO
- см. в коде
- Сделать карточку данных

# Шпаргалка
## SQLite
- `.tables` - показать список таблиц
- `.mode table` - рисовать ASCII таблицу в ответ на запрос
