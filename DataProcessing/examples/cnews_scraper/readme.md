# Пример скрапера для сайта cnews.ru

Скрапер ориентирован на обработку XML-карты сайта. Сценарий подразумевает три этапа — загрузка метаданных, заполнение очереди ссылок и скачивание HTML.

### Структура
- `config.py` — параметры: адрес сайта, папка данных, настройки базы sqlite и задержка между запросами.
- `get_meta.py` — получает `robots.txt`, находит ссылку на sitemap и скачивает его в `data/`. Также выводит количество ссылок и примерную информацию.
- `make_pages_queue.py` — парсит скачанную карту, сохраняет уникальные `loc` в таблицу `pages_urls` и помечает их состоянием `new`.
- `download_pages.py` — извлекает все значения `state = 'new'`, скачивает HTML через `httpx`, сохраняет содержимое с хешем и временем в `pages_content`, переводит строку в `downloaded`.

### Подготовка
1. Создайте виртуальное окружение и установите зависимости:
   ```
   python -m venv .venv
   .venv/bin/pip install -r requirements.txt
   ```
2. Убедитесь, что существует папка `data/`. Именно туда скрипты создают `robots.txt`, `sitemap.xml` и базу `database.sqlite`.
3. При необходимости скорректируйте `config.py` — например, увеличьте `DELAY`, если сайт начинает отказываться.

### Использование
1. `python get_meta.py` — скачивает `robots.txt`, определяет sitemap и сохраняет его. После завершения внутри `data/` появится `robots.txt`, `sitemap.xml`.
2. `python make_pages_queue.py` — парсит `data/sitemap.xml`, записывает ссылки в БД SQLite, в таблицу `pages_urls`.
3. `python download_pages.py` — проходит по таблице `pages_urls`, скачивает HTML, сохраняет их в отдельную таблицу в этой же БД, обновляет статус каждой записи в исходной таблице.

### Состояние базы
- `pages_urls` — `id`, `url`, `type`, `state`, `note`. `state` принимает `new`, `downloaded`.
- `pages_content` — `id`, `content`, `url`, `hash`, `datetime`. Хеш вычисляется через `blake2s`.
Проверить статистику можно командой:
```sh
sqlite3 data/database.sqlite "SELECT state, count(*) FROM pages_urls GROUP BY state;"
```

### Расширения
- Подключите новый парсер нужного раздела — ищите ссылки из `pages_content` по ключевым блокам, сохраняйте в другой таблице.
- Используйте `httpx.AsyncClient`, если нужно массово параллельное скачивание (учитывайте `robots.txt`).


# TODO
