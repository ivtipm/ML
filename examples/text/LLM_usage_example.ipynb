{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# установка пакетов\n",
    "!pip install --upgrade ollama\n",
    "!pip install --upgrade llama-index\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
    "# OLLAMA_URL = \"http://172.17.0.1:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обращение к серверу с LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama сервер работает!\n",
      "{\"models\":[{\"name\":\"rscr/vikhr_nemo_12b:Q4_K_M\",\"model\":\"rscr/vikhr_nemo_12b:Q4_K_M\",\"modified_at\":\"2024-12-26T20:48:38.657378091+09:00\",\"size\":7477220858,\"digest\":\"a8149e66bb9329ea549a30443f850ed69456a443d156d9ec7f5ceb7fe380a1e2\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"12.2B\",\"quantization_level\":\"Q4_K_M\"}},{\"name\":\"qwen2.5:7b\",\"model\":\"qwen2.5:7b\",\"modified_at\":\"2024-12-26T20:48:37.576363399+09:00\",\"size\":4683087332,\"digest\":\"845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"qwen2\",\"families\":[\"qwen2\"],\"parameter_size\":\"7.6B\",\"quantization_level\":\"Q4_K_M\"}},{\"name\":\"mannix/llama3.1-8b-abliterated:q6_k\",\"model\":\"mannix/llama3.1-8b-abliterated:q6_k\",\"modified_at\":\"2024-12-26T20:48:36.441347972+09:00\",\"size\":6596021404,\"digest\":\"77f40eeac40ba75b7adccff7dd17fbf41f05a4cb72ea4daa2f1100b029972fa4\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q6_K\"}},{\"name\":\"llama3.2:3b\",\"model\":\"llama3.2:3b\",\"modified_at\":\"2024-12-26T20:48:34.062315625+09:00\",\"size\":2019393189,\"digest\":\"a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"3.2B\",\"quantization_level\":\"Q4_K_M\"}},{\"name\":\"llama3.2:1b\",\"model\":\"llama3.2:1b\",\"modified_at\":\"2024-12-26T20:48:32.948300475+09:00\",\"size\":1321098329,\"digest\":\"baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"1.2B\",\"quantization_level\":\"Q8_0\"}},{\"name\":\"llama3.1:8b\",\"model\":\"llama3.1:8b\",\"modified_at\":\"2024-12-26T20:48:31.845285471+09:00\",\"size\":4920753328,\"digest\":\"46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_K_M\"}},{\"name\":\"lakomoor/vikhr-llama-3.2-1b-instruct:fp16\",\"model\":\"lakomoor/vikhr-llama-3.2-1b-instruct:fp16\",\"modified_at\":\"2024-12-26T20:48:30.719270152+09:00\",\"size\":2479596729,\"digest\":\"acbf9b9107175405c302dcb84128c096b84121f036a2d564dd9e2d46c1bf4a87\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"1.2B\",\"quantization_level\":\"F16\"}},{\"name\":\"gemma:2b\",\"model\":\"gemma:2b\",\"modified_at\":\"2024-12-26T20:48:29.579254639+09:00\",\"size\":1678456656,\"digest\":\"b50d6c999e592ae4f79acae23b4feaefbdfceaa7cd366df2610e3072c052a160\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"gemma\",\"families\":[\"gemma\"],\"parameter_size\":\"3B\",\"quantization_level\":\"Q4_0\"}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "try: \n",
    "    response = requests.get(OLLAMA_URL)\n",
    "    if response.status_code == 200:         print(\"Ollama сервер работает!\")\n",
    "    else:                                   print(f\"Сервер ответил с кодом: {response.status_code}\")\n",
    "except requests.ConnectionError:            print(\"Не удалось подключиться к Ollama серверу. Убедитесь, что он запущен.\")\n",
    "\n",
    "# список моделей\n",
    "reps = requests.get( OLLAMA_URL + \"/api/tags\"  )\n",
    "print(reps.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ от сервера: The question you're referring to is a famous one from Douglas Adams' science fiction series \"The Hitchhiker's Guide to the Galaxy.\" In the book, a supercomputer named Deep Thought is asked to find the answer to the ultimate question of life, the universe, and everything. After thinking for 7.5 million years, Deep Thought finally reveals that the answer is... \n",
      "\n",
      "**42**\n",
      "\n",
      "However, the characters soon realize that they don't actually know what the ultimate question is, so the answer isn't really helpful in the grand scheme of things.\n",
      "\n",
      "Do you want to discuss this with me further or move on to something else?\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "       \"model\": \"llama3.2:1b\",\n",
    "       \"prompt\": \"What is the answer to the Ultimate Question of Life, the Universe, and Everything?\",\n",
    "    #    \"prompt\": \"What model are you?\",\n",
    "       \"stream\": False,\n",
    "}\n",
    "\n",
    "# Отправляем POST-запрос\n",
    "response = requests.post(OLLAMA_URL + \"/api/generate\", json=data)\n",
    "\n",
    "# Проверяем, что запрос прошел успешно\n",
    "if response.status_code == 200: print(\"Ответ от сервера:\", response.json()['response'])\n",
    "else:                           print(\"Ошибка при запросе:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"Non dies sine committere\" from Latin to English is:\n",
      "\n",
      "\"No days without working\"\n",
      "\n",
      "Or, more literally:\n",
      "\n",
      "\"No idle days\" or \"No free days\".\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "mes = 'Please translate from latin: Non dies sine committere'\n",
    "\n",
    "response = ollama.chat(model='llama3.2:1b',\n",
    "                      messages=[{'role': 'user', 'content': mes}])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain_community.chat_models -> ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**langchain** — это модульный фреймворк для работы с множеством языковых моделей, векторных баз данных и других инструментов обработки текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# адрес и порт сервера Ollama\n",
    "ollama_base_url = \"http://localhost:11434\"\n",
    "\n",
    "# Инициализация модели\n",
    "llm = ChatOllama(model=\"llama3.2:1b\", base_url=ollama_base_url)\n",
    "\n",
    "# Функция для получения ответа от модели\n",
    "def get_response(user_input):\n",
    "   messages = [HumanMessage(content=user_input)]\n",
    "   return llm.invoke(messages).content\n",
    "\n",
    "# Пример использования\n",
    "user_input = \"What is LD50 of coffee?\"\n",
    "response = get_response(user_input)\n",
    "print(\"Ответ:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# создание простого промпта\n",
    "# todo: ...\n",
    "\n",
    "# структура промпта\n",
    "template = \"\"\"\n",
    "System: {system_message}\\n\\n\n",
    "Human: {user_message}\\n\\n\n",
    "Context: {context}\\n\\n\n",
    "AI: Please respond to the human's message.\n",
    "\"\"\"\n",
    "\n",
    "# Последний раздел (AI) можно использовать по необходимости, когда запрос может быть не вполне очевиден или требует дополнительных уточнений.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"system_message\", \"user_message\", \"context\"]           # названия переменных из текста template\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt.format(\n",
    "    system_message=\"You are a helpful AI assistant.\",\n",
    "    user_message=\"What's the capital of France?\",\n",
    "    context=\"France is a country in Western Europe.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSystem: You are a helpful AI assistant.\\n\\n\\nHuman: What's the capital of France?\\n\\n\\nContext: France is a country in Western Europe.\\n\\n\\nAI: Please respond to the human's message.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "       \"model\": \"llama3.2:1b\",\n",
    "       \"prompt\": formatted_prompt,\n",
    "    #    \"prompt\": \"What model are you?\",\n",
    "       \"stream\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(f\"{OLLAMA_URL}/api/generate\", json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://python.langchain.com/docs/integrations/llms/ollama/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
