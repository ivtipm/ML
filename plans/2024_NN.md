# Нейросетевые технологии анализа данных


# Остальные темы
- RAG системы
- ИИ-агенты на основе LLM



# Занятие 17. Модификации градиентного спуска. Инициализации весов.
18 декабря

# Занятие 16. GPT. BERT и т.п. Использование LLM.
11 декабря

# Занятие 15. GPT. BERT и т.п.
4 декабря
- Повторение: как работал Transfer Learning в CV? seq2seq. Transformer. Self Attention.
- BERT

Слайды: https://docs.google.com/presentation/d/1Kpk3DcHNf1iOMcggF701VorNuI3csqcuU9G69mUxtZU/edit?usp=sharing

Пример классификации текстов моделью GPT2 (distilgpt2, huggin face transformers, pytorch)
https://drive.google.com/file/d/1euX-lo9QgNEpNats9Pkl1YVvY7IdQTnk

Пример использования BERT для создания векторных представлений текстов: https://colab.research.google.com/drive/1Aacg8tUXXNICQ0SetvqoIPD7GXnm0Ymx?usp=sharing

# Занятие 14. Transformers. GPT.
28 ноября
- Повторение. Transfer Learing (перенос знаний). Encoder-Decoder. Attention (Encoder-Decoder).
- BPE
- Self-Attention
- Transformers. GPT

Слайды: https://docs.google.com/presentation/d/1Kpk3DcHNf1iOMcggF701VorNuI3csqcuU9G69mUxtZU/edit?usp=sharing

Пример классификации текстов моделью GPT2 (distilgpt2, huggin face transformers, pytorch)
https://drive.google.com/file/d/1euX-lo9QgNEpNats9Pkl1YVvY7IdQTnk


### Домашнее задание 7 (продолжение). Классификация текстов GPT и BERT
1. Решите задачу классификации текстов используя GPT-2. Используйте не обученную модель и предварительно обученную модель.
2. Решите задачу классификации текстов используя BERT.
3. *Бонус: создайте веб-приложение*
4. *Бонус: создайте образ docker*

# Занятие 12-13. Задача Seq2Seq. Архитектура Encoder-Decoder. Attention.
21 ноября
- Повторение. RNN (принцип работы, скрытое состояние, размерности) . Векторные представления (embeddings) слов. Изменение размерности векторного пространства линейной операцией.
- Генерация последовательностей по методу Seq2Seq. Архитектура Encoder-Decoder на примере задачи машинного перевода. Метрика BLEU. Beam Search.
- Механизм внимания (attention). Самонаправленное внимание (self-attention).

Слайды: https://docs.google.com/presentation/d/1Kpk3DcHNf1iOMcggF701VorNuI3csqcuU9G69mUxtZU/edit?usp=sharing

# Занятие 12. LSTM. GRU.
14 ноября

# Занятие 11. RNN. Эмбеддинги текста
7 ноября
- RNN. Принцип работы. Обучение. Несколько RNN нейронов.
- Эмбеддинги слов. Готовый словарь и создание собственного.
- Архитектура сети для классификации текстов по тональности.
- Пример Keras. pytorch.
- Слайды: https://docs.google.com/presentation/d/1cU08j3zl3rpBa00hjsP5Hrz8H-9-WR5Y4Z22K_N7mQU/edit?usp=sharing
- Примеры:
- https://colab.research.google.com/drive/1XXQgLdECny8MvO8VVcEn3-cM1TW8pdHK — основы RNN
- https://colab.research.google.com/drive/1-DR0TUO-2WPT9xp8LaC6OOJxKbnbG6NO — применение RNN для классификации текстов
### Домашнее задание 7. Классификация текстов RNN сетями
1. Решите задачу классификации текстов используя нейросеть на основе RNN
2. Сравните результаты с классическими моделями и представлении текста TF-IDF
3. *Бонус: реализуйте механизм внимания.*

# Занятие 9-10. Временные ряды. SARIMA?. Facebook prophet?
24-31 октября
- Повторение: регрессия, метрики качества, функции потерь. математическое ожидание и дисперсия.
- Введение в анализ временных рядов: https://www.figma.com/file/1DzcdjkodaIvQPFBji5UVu/ML?type=design&node-id=37%3A994&mode=dev
- Примеры: 
    - Анализ, разложения, примитивные модели, AR, кросс-валидация, sktime:
      https://colab.research.google.com/drive/1hgFTqkfu37v7PQAR9JYcH610FJdXOP-b?usp=sharing  
    - SARIMAX, Facebook Prophet: https://colab.research.google.com/drive/1_9vRiJ4BPhQMXd1PAjKK61ARjPp46IdY?usp=sharing
    - См. также адаптирование классических ML моделей для временных рядов

**Ссылки*
- (Современные модели прогнозирования типа TimesNet и TimeGPT // Курс «Machine Learning. Advanced»)[https://youtu.be/tj57sozJUwE?si=cv7Q2eEAKnueBQPu]


### Домашнее задание 7. Анализ временных рядов.
1. Изучите датасет https://kaggle.com/competitions/store-sales-time-series-forecasting (datasets/store-sales-time-series-forecasting.zip)
    - Какие временные ряды там присутствуют? Что они представляют?
    - Проанализируйте 1 или больше временных рядов для товаров в магазине. Сделайте проверку на стационарность. Выделите трнед. Сезонность.
2. Обучите модели, сделайте предсказания
    - SARIMA(x)
    - Facebook Prophet
    - Нейросеть на основе RNN
3. Напишите пояснения и выводы.

# Занятие 8. Распознавание лиц?
17 октября
- Повторение. Детекция. Сегментация, виды. Как обычно представляются объекты в МО? Скалярное произведение векторов, когда оно максимально? Софтмакс.
- Распознавание лиц
- Слайды: https://docs.google.com/presentation/d/105hLZaG1FQGqiL6Z_QDsk4rRm3gt0RSvm5EilzJ71R8/edit?usp=sharing
- Пример: https://colab.research.google.com/drive/1qn4PrfSFq1NHdplxtMA1w6jehqYNLBgr?usp=sharing

### Домашнее задание 6. Распознавание лиц
- Создайте веб-интерфейс для предварительно обученной модели распознавания лиц
- *Бонус: создайте docker-образ*



# Занятие 7. Сегментация
10 октября
- Сегментация изображений. Виды. FCN. Unet. Transposed Convolution. Dilated Convolution.
- Слайды: https://docs.google.com/presentation/d/105hLZaG1FQGqiL6Z_QDsk4rRm3gt0RSvm5EilzJ71R8/edit?usp=sharing
- Пример: https://colab.research.google.com/drive/1qn4PrfSFq1NHdplxtMA1w6jehqYNLBgr?usp=sharing



### Домашнее задание 5. Сегментация
- Создайте веб-интерфейс для предварительно обученной модели сегментации
- Пользователь может загрузить изображение,
- выбрать параметры детекции: набор классов
- *Бонус: создайте docker-образ*


# Занятие 5-6. Детекция. Сегментация
3 октября
- Задача детекции объектов на изображении. YOLO
    - Слайды: https://docs.google.com/presentation/d/105hLZaG1FQGqiL6Z_QDsk4rRm3gt0RSvm5EilzJ71R8/edit?usp=sharing
    - Пример: https://colab.research.google.com/drive/1qn4PrfSFq1NHdplxtMA1w6jehqYNLBgr?usp=sharing
- Семантическая сегментация?

### Домашнее задание 4. Детекция
- Создайте веб-интерфейс для предварительно обученной модели детекции (YOLO, RetinaNet и т.п.)
- Пользователь может загрузить изображение,
- выбрать параметры детекции: набор классов, количество детектируемых объектов на каждый класс, минимальный уровень уверенности в детекции объекта
- Решите задачу классификации используя предварительно обученную модель CLIP
- *Бонус: создайте docker-образ*
- *Бонус: дообучите готовую модель решать задачу детекции для новых классов*
- *Бонус: детекция и распознавание номерных знаков?*



# Занятие 4. ViT, CLIP ...
26 сентября
- ViT
- OneShot классификация с CLIP,




## Домашнее задание 3. Классификация изображений
Решите задачу классификации изображений.
Используйте датасет: Animal Faces-HQ (AFHQ), consists of 16,130 high-quality images at 512×512 resolution.
    - Источник данных: https://www.kaggle.com/andrewmvd/animal-faces
    (Для загрузки необходима регистрация на kaggle, в верхней части страницы кнопка download для загрузки архива. - Альтернативно: загрузка через kaggle api напрямую в colab: https://colab.research.google.com/github/corrieann/kaggle/blob/master/kaggle_api_in_colab.ipynb )
    - Или ищите прямую ссылку на архив с данными в дискорде.

Можно предложить свой датасет аналогичный по объёму, разрешению изиображений или более сложный.

Выполните все пункты как в предыдущем задании.



# Занятие 3. СNN. Архитектуры: AlexNet, VGG, Resnet, Inception
19 сентября
- AlexNet, VGG, проблема затухающего градиента, ResNet (Skip Connection), Inception
- Использование предварительно обученных сетей (transfer Learning).
- Пример: https://colab.research.google.com/drive/1kqSiP9IpPmW8dw9NxV5Sm5pMLMqfINZb?usp=sharing
- [слайды](https://docs.google.com/presentation/d/1i41kqGwZqW_sSRz9Bopoq7AZf_Zuo6OaRSeCKYIH_80/edit?usp=sharing)




# Занятие 2. CNN. Аугментация. Разбор примера.
12 сентября

1. Повторение
2. CNN. Архитектура нейросети для классификации изображений. Примеры PyTorch, Keras. [пример](https://colab.research.google.com/drive/1kqSiP9IpPmW8dw9NxV5Sm5pMLMqfINZb?usp=sharing) 


## Задание 2. CNN на основе примера классификации рукописных цифр
Пример: https://colab.research.google.com/drive/1kqSiP9IpPmW8dw9NxV5Sm5pMLMqfINZb#scrollTo=ePxpxkivl4Uy&line=26&uniqifier=1
1. Исследуйте данные.
1. Подберите гиперпараметры моделей на PyTorch и Keras
    * Гиперпараметры обучения: количество эпох, шаг обучения, количество и конфигурация слоёв. Не забывайте про нормализацию и dropout. 
    - Объясните, почему выбрали именно такую архитектуру. Прокомментируйте все слои, их гиперпараметры. Задайте важные гиперпараметры в явном виде вместо неявного использования значения по умолчанию.
    - Какую функцию потерь использовали? Почему?
    - Сколько параметров получилось? На каких слоях больше всего обучаемых параметров?
    - Как максимизировали качество модели? Как минимизировали размер модели?
    - Как контролировали переобучение? Как боролись с ним?
    - Как можно ускорить обучение?
1. Используйте готовые модели из библиотек Keras и Pytorch.
    - Что такое transfer learning?
    - Какие варианты использования (обучения) этих моделей существуют?
    - Почему здесь предварительно обученная на другом датасете нейросеть (кроме последних слоёв) может работать на данном датасете? 
1. Используйте аугментацию изображений (для всех моделей). Не изменяйте изображения слишком сильно. 
    - Зачем нужна аугментация? Как она работает? 
    - Как влияет на время обучения? На обобщающую способность модели?
    - Какие характеристики изображения можно изменять? Как они влияют на качество модели и время обучения?
1. Отслеживайте обучение через сервис WandB. Можно использовать локальные инструменты для отслеживания вместо WandB
    - Записывайте гиперпараметры модели, количество параметров модели, гиперпараметры обучения.
    - Стройте графики во время обучения (для обучающей и валидационной выборок).
    - Записывайте качество модели на тестовой выборке.
    - Постройте график в параллельных координатах, по которому можно оценить влияние гиперпараметров на качество предсказания.
1. Структурирование работы, комментарии и справка. Дополните код поясняющими комментариями.
    - Описывайте общий алгоритм работы с данными и моделями.
    - Поясняйте используемые классы, функции и их параметры.
    - Приводите ссылки на документацию, справочные материалы, поясняющие изображения и т.п.
    - Запишите ответы на вопросы в python-тетрадке
    - *Дополнительный балл за создание схемы нейросети (потока данных через нейросеть)*
1. Выводы. 
    - Опишите эксперименты. Описывайте не только результаты, но и объясняйте их влияние на качество и размер модели. Описывайте как влияют гиперпараметры на точность, время обучения и т.п.
    - Пишите в выводах как ещё можно [попробовать] улучшить качество модели или оптимизировать её размер.
1. Используйте предварительно сохранённую нейросеть для классификации.
1. *Дополнительный балл за веб-приложение в качестве интерфейса для модели обученной распознавать числа.*
1. *Дополнительный балл за создание образа docker: 
    - балл за сервер, отвечающий на запросы по API
    - балл за сервер, отвечающий за сервер, на котором доступен веб-интерфейс* [[пример](../examples/docker-ML-web)]
    - балл за систему из API сервера и веб-сервера* [[пример](../examples/docker-api/)]
1. *Дополнительный балл за использование похожего, но более сложного датасета.*
git 
Пример создания интерфейса на основе Gradio: https://colab.research.google.com/drive/1SzAlYDLpjf65nnRbTVRtFxUWuyJnA8_s?usp=sharing#scrollTo=yGX_NM3aoN7V



# Занятие 1. Повторение. Введение в CNN
5 сентября

1. Повторение. [слайды](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing)
    - Задачи МО. Классификация методов МО.
    - Метрики качества.
    - Обучение. Функция ошибок. Метод градиентного спуска. [pdf](https://github.com/ivtipm/ML/blob/main/Gradient%20Descent.pdf)
    - Полносвязные нейросети. [слайды](https://docs.google.com/presentation/d/1YCJhQIj2BV42sDLKtxmytcYW5W39EJceYfqrYT7bKNo/edit?usp=sharing), [пример](https://colab.research.google.com/drive/1YtK4an7UAhnxTmhmQzZd6Eo3esfv6TL3?usp=sharing)
2. CNN. Основные идеи. [слайды](https://docs.google.com/presentation/d/1i41kqGwZqW_sSRz9Bopoq7AZf_Zuo6OaRSeCKYIH_80/edit?usp=sharing)
    - Представление изображений в виде тензора
    - Операция свёртки, свёртка для одноканального и трёхканальгого изображения, гиперпараметры операции
    - Пулинг
    - Архитектура нейросети для получения векторных представлений изображений


## Задание 1. Градиентный спуск
**Вычисления вручную**
1. Выберете функцию одной переменной, по сложности сопоставимую с $L(x) = 3 x ^ 2 - sin(x) + e^x - 5$
2. Изобразите график функции
3. Сделайте 3 или больше шага метода градиентного спуска. Изобразите их на графике.
Можно записывать вычисления в LaTeX в Jupyter Notebook [ шпаргалка: [1](https://forketyfork.medium.com/latex-math-formulas-a-cheat-sheet-21e5eca70aae), [ 2 ](https://nbviewer.org/github/dingran/latex-ipynb/blob/master/latex-cheatsheet.ipynb) ]
**Программа**
1. Создайте программу на Питоне для минимизации выбранной функции. Задайте условие остановки на основе изменения функции.
2. Постройте график функции, отметьте на нём шаги метода.
- Интерактивные графики, уменьшение размерности, веб-формы (plotly, PCA, gradio): colab.research.google.com/drive/1SzAlYDLpjf65nnRbTVRtFxUWuyJnA8_s 
***Бонус*** (не обязательно)
1. Для программы на Питоне, минимизирующей выбранную функцию реализуйте модификации градиентного спуска:
    - ГД с моментом
    - ГД с моментом Нестерова