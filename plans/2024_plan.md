# О курсе
Курс посвящён машинному обучению, направлению искусственного интеллекта, где задачи решаются на основе анализа данных. Нередко большого количества данных. В курсе будут рассмотрены основные понятия машинного обучения, способы анализа и визуализации данных, преобразования данных в вид, подходящий для моделей машинного обучения. Будут рассмотрены классические модели машинного обучения, начиная от регрессий, заканчивая градиентным бустингом; методы обучения и оптимизации качества этих моделей. В конце курса будут рассмотренные полносвязные нейросети (многослойные персепторн). 

Задания будут посвящены преимущественно обработке табличных и текстовых данных на примере синтетических и реальных наборов данных.

Рассматриваемые не нейросетевые методы МО применяются на практике, не смотря на большую популярность нейросетей. Курс можно считать как самостоятельным, так и длинным, но во многом обязательным, введением в курс нейросетевого анализа данных. Последний будет проходить в следующем семестре.

**См. также** план предыдущего курса, ссылки и примеры на главной странице, записи лекций предыдущего курса в дискорде.

## Требования для успешного усвоения курса.

Для успешного освоения курса необходимы знания языка программирования Python, основ линейной алгебры, математического анализа и методов оптимизации.

### Материалы для ознакомления перед началом курса
0. Шпаргалка по Python https://miro.com/app/board/uXjVNQC1rq8=/?share_link_id=860218532001
0. Теория вероятностей и математическая статистика
    - [raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_1.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_1.pdf)
   - [raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf)
   - [raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf)
   - Примеры распределений и их вид в зависимости от параметров: [seeing-theory.brown.edu/probability-distributions/index.html#section3](https://seeing-theory.brown.edu/probability-distributions/index.html#section3)
   - Среднее значение (average) и дисперсия (variance), интерактивный пример: [seeing-theory.brown.edu/basic-probability/index.html#section3](https://seeing-theory.brown.edu/basic-probability/index.html#section3)

0. Математическая статистика, основы numpy, pandas, matplotlib
    1. [Математические модели и вычислительные методы обработки экспериментальных данных](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf)
    1. 6 способов значительно ускорить pandas с помощью пары строк кода: https://habr.com/ru/articles/503726/, https://habr.com/ru/articles/504006/


# План
# 1. Введение в машинное обучение. Математическая статистика. NumPy. Pandas.
1. Введение: [docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing)
1. Теория вероятностей и математическая статистика:
    - [raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf)
    - [raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf)

### Задание 0. NumPy и основные понятия математической статистики
Не используйте никакие методы numpy array кроме sum и abs. Не используйте циклы, генераторы, функции стандартной библиотеки Питона sum, zip и map, используйте векторные вычисления numpy.

Используйте Google Colaboratory.

1. Для функций ошибок и величин, широко используемых в анализе данных и машинном обучении, напишите функции:
    1. для вычисления стандартного отклонения случайной величины;
    1. для вычисления ошибок MSE, MAE, accuracy, precision, recall, f1 score. 
    Функции должны принимать два параметра y_pred, y_true. См. аналогичные функции из библиотеки sklearn;
    1. для вычисления функции softmax на нескольких переменных, используйте `numpy.exp`;
    1. вычисления линейного коэффициента корреляции;
    1. для вычисления предсказания для нескольких объектов по заданному коэффициентами уравнения линейной регрессии;
    1. подбора коэффициентов линейной регрессии, с использованием метода наименьших квадратов и функцией `minimize` модуля `scipy.optimize`.
1. Напишите тесты для этих функций;
1. Пишите поясняющие и документирующие комментарии;
1. Напишите формулы в формате LaTeX для всех функций, пояснения к формулам.

Для проверки кода, связанного с регрессией можно воспользоваться функцией `make_regression` из библиотеки Sklearn. Она создаёт синтетические данные для задачи регрессии. 

См. шпаргалку по Python: https://miro.com/app/board/uXjVNQC1rq8=, учебное пособие [Математические модели и вычислительные методы обработки экспериментальных данных](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf)

Задание может быть дополнено.



# Лекция 2. Основы Google Colaboratory. Синтетические данные. EVA. Линейная регрессия.

0. учебное пособие [Математические модели и вычислительные методы обработки экспериментальных данных](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf) - о построение графиков, numpy, pandas.

1. https://colab.research.google.com/drive/18YGaumubomt-Rtg_9_VT_nf1GwbMlEx8 - синтетические данные. EVA. Линейная регрессия.

2. [EDA, предобработка, кодирование colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR](https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR)

2. [Линейная регрессия. МНК. Метод макс.правдоподобия. sklearn.ipynb colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx](https://colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx?usp=sharing)


### Задание 1. Основы Google colab. Синтетические данные. Простое исследование данных (EVA). Линейная регрессия.
1. Освойтесь в google colaboratory
    - Как создавать ячейки? Какие виды ячеек бывают?
    - Какие способы запуска ячеек есть? Как ячейки с кодом влияют друг на друга?
    - Что такое markdown? Как создавать заголовки (и просматривать их в Google Colaboratory), делать текст жирным, курсивом, приводить фрагменты кода с подсветкой синтаксиса, приводить формулы в записи LaTeX?
    - Как делиться проектом (тетрадкой - notebook)?
    - Приведите ссылку на задание, текст задания.
1. Изучите пример 1 из лекции. 
1. Создайте синтетический набор данных для решения задачи линейной регрессии. Количество объектов должно быть меньше 10к, количество независимых признаков - 4 или больше, среди них 1 или 2 не должны влиять на  целевую переменную. 
1. Исследуйте данные. 
    - Изучите числовые характеристики всех признаков через 7-point summary. Используйте метод `describe()` класса pandas.DataFrame. Напишите пояснения для этих характеристик.
    - Постройте диаграммы размаха для всех признаков (seaborn.boxplot), скрипичные диаграммы (violinplot). Напишите пояснения. Есть ли выбросы?
    - Разберитесь как задать размер полотна (изображения) для графиков? Как построить несколько графиков, на одном полотне, но с разными осями? Постройте несколько диаграмм размаха 
    - Постройте гистограмму для целевого признака. Используйте рекомендованное библиотекой число столбцов, задайте много столбцов. Опишите, как такая гистограмма устроена. Видны ли на гистограмме с большим количеством столбцов аномалии в данных?
    - Вычислите матрицу корреляции, сделайте для неё тепловую карту. Напишите, какие независимые признаки влияют на целевой сильнее всего? Есть ли математическая зависимость между независимыми признаками? Как это влияет на качество уравнения линейной регрессии?
    - Постройте попарные дигаммы рассеяния (seabor.pairplot). Что показывает такая диаграмма? Для какого распределения целевого признака предсказания уравнения линейной регрессии будут точнее всего? Постройте для этого признака отдельную диаграмму рассеяния (seaborn.jointplot).
    - Дополнительно используйте пакет plotly для построения двухмерной и трёхмерной диаграммы рассеяния. На диаграмме должен быть целевой признак, и два наиболее значимых независимых признака.
    - Бонус: дополнительно постройте диаграммы на свой выбор. 
2. Разделите исходные данные на выборку для обучения (train) и отложенную выборку (test) для проверки результатов обучения. Оцените качество модели на этих выборках. Почему результаты могут отличаться?
    - Вычисляйте ошибку MSE, MAE.  Используйте встроенные в sklearn функции. Используйте функции, созданные в предыдущем задании.
    - Вычислите коэффициент детерминации R2. О чём он говорит?  
    - upd: приведите функцию потерь, пояснения к ней. Зачем нужна эта функция?    
3. Напишите пояснения и комментарии к коду. Поясняйте общий алгоритм, смысл действий, понятия (std, mean, ...) параметры вызываемых функций, записанные в коде формулы (приведите их в LaTeX). Комментарии к коду можно оставлять в ячейках с кодом, остальные пояснения можно давать в ячейках с текстом.


Задание дополнено!


# Лекция 3. Функция потерь
1 марта
0. Повторение. Классификация vs регрессия. Показатели качества модели регрессии. Функция потерь для линейной регрессии.
1. Подбор параметров уравнения линейной регрессии. Метод наименьших квадратов. Понятие функции потерь. Понятие градиентного спуска. Аналитическое решение для коэффицентов линейной регрессии
 [Линейная регрессия. МНК. Метод макс.правдоподобия. sklearn.ipynb colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx](https://colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx?usp=sharing)



# Лекция 4. Задача классификации. Логистическая регрессия. Показатели качества классификации.
8 марта
- Повторение
- Линейная разделимость классов. Сигмоида. Вероятностная модель; LogLoss.
- [colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh?usp=sharing](https://colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh?usp=sharing)
- Показатели качества классификации: [docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing)



### Домашнее задание 2. Классификация на синтетических данных.
0. Приведите ссылку на задание, текст задания.
1. Сгенерируйте данные. Создайте 5 или больше независимых признака, в том числе 1-2 лишних, 2 класса. [ [пример](https://colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh#scrollTo=57PKhQ2Ylni1) ]. Создавайте не более 10 тысяч объектов.
1. Исследуйте данные. 
    - Изучите числовые характеристики всех признаков через 7-point summary. Используйте метод `describe()` класса pandas.DataFrame. Напишите пояснения для этих характеристик.
    - Постройте диаграммы размаха для всех признаков (seaborn.boxplot), скрипичные диаграммы (violinplot). Напишите пояснения. Есть ли выбросы?
    - Разберитесь как задать размер полотна (изображения) для графиков? Как построить несколько графиков, на одном полотне, но с разными осями? Постройте несколько диаграмм размаха 
    - Постройте гистограмму для целевого признака. Используйте рекомендованное библиотекой число столбцов, задайте много столбцов. Опишите, как такая гистограмма устроена. Видны ли на гистограмме с большим количеством столбцов аномалии в данных?
    - Вычислите матрицу корреляции, сделайте для неё тепловую карту. Напишите, какие независимые признаки влияют на целевой сильнее всего? Есть ли математическая зависимость между независимыми признаками? Как это влияет на качество уравнения логистической регрессии? Имеет ли смысл считать коэф. корреляции для категориального признака?
    - Постройте попарные дигаммы рассеяния (seabor.pairplot). Кодируйте класс цветом. Что показывает такая диаграмма? Постройте для этого признака отдельную диаграмму рассеяния (seaborn.jointplot).
    - Дополнительно используйте пакет plotly для построения двухмерной и трёхмерной диаграммы рассеяния. На диаграмме должен быть целевой признак, и два наиболее значимых независимых признака.
    - Бонус: дополнительно постройте диаграммы на свой выбор. 
1. Обучите модель логистической регрессии. Приведите общую формулу, пояснения для неё. Приведите функцию потерь, пояснения к ней. Зачем она нужна?
1. Оцените её на отложенной выборке. Вычислите accuracy (общую точность), точность (precision), recall (полноту), меру f1. Опишите эти показатели. 
3. Напишите пояснения и комментарии к коду. Поясняйте общий алгоритм, смысл действий, понятия (std, mean, ...) параметры вызываемых функций, записанные в коде формулы (приведите их в LaTeX). Комментарии к коду можно оставлять в ячейках с кодом, остальные пояснения можно давать в ячейках с текстом.



# Лекция 5. kNN. SVM.
15 марта
- https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR?usp=sharing - EVA
- https://colab.research.google.com/drive/1oh7-ID00MN-AoJtAm4uL-bj0sqf5ieuk - kNN



### Домашнее задание 3.
3. Используйте метод кросс-валидации для обучения и оценки модели на небольшой выборке.


### Домашнее задание 4. 
1. Добавьте в работы 1 и 2 модели:
    - KNN
    - SVM
    - Решающее дерево

2. Подберите гиперпараметры этих моделей. Используйте класс GridSearchCV.




# Лекция 6. Поиск по сетке гиперпараметров. Решающие деревья. CRISP-DM.
22 марта\
0. Повторение
1. Поиск по сетке гиперпараметров. [[Теория и пример](https://colab.research.google.com/drive/1oh7-ID00MN-AoJtAm4uL-bj0sqf5ieuk?usp=sharing)]
2. Решающие деревья [[colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg#scrollTo=o3dx6WQiX1m7](https://colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg#scrollTo=o3dx6WQiX1m7)]
3. EVA. CRISP-DM. Разбор примера. [ [https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR?usp=sharing#scrollTo=b4n53izzoxC3] ]
    - начальный анализ
    - пропуски и неизвестные значения, выбросы, аномалии
    - кодирование: one_hot, label encoding, ручное кодирование по словарю
    - масштабирование


### Домашнее задание 5. EVA и классификация на реальных данных
0. Приведите ссылку на задание, текст задания.
1. Используйте один из предложенных датасетов:
    - вариант 1. https://raw.githubusercontent.com/ivtipm/ML/main/datasets/churn.csv (есть на Kaggle)
    - вариант 2 (сложнее, но даёт примерно в 1.5 раза больше баллов). https://archive.ics.uci.edu/ml/datasets/adult, https://www.kaggle.com/wenruliu/adult-income-dataset?select=adult.csv
    - можно обработать оба
или предложите свой датасет, сопоставимый по количеству и качеству данных с рекомендованным. Используйте свой датасет после согласования с преподавателем.
1. Проведите разведочный анализ данных.
    - 7 point summary,
    - Пропуски, дубликаты
    - диаграммы размаха, скрипичные диаграммы (violin plot),
    - матрица корреляции,
    - попарные диаграммы рассеяния,
    - столбчатую диаграмму для целевого признака, что можно сказать про баланс классов?
    - Коротко опишите данные. Сколько признаков? Какие признаки сильнее всего влияют на целевой признак? Есть ли выбросы? Какие распределения имеют признаки (нормальное, равномерное и т.п.)?
    - Удалите лишние признаки, кодируйте нечисловые признаки, удалите выбросы, масштабируйте признаки. Поясняйте действия, описывайте зачем их производите.
    - upd: Бонус: интерактивные диаграммы на основе plotly: гистограммы для важны?
    - Опишите данные: какие данные даны? Сколько признаков и объектов? Есть ли пропуски, дубликаты, выбросы
    -  *В начале работы рекомендуется удалить все неудобные (много пропусков, нужно кодировать, есть ошибки и т.п.) признаки и построить простую модель. Записывайте как проводили обработку данных и результаты. Далее, можно более тщательно изучить и подготовить данные, снова обучить модель и сравнить результаты с предыдущим разом.*
1. Обучите модели:
    - Логичтическую регрессию
    - SVM
    - kNN
    - Решающее дерево
1. Подберите гиперпараметры моделей.
1. Опишите модели, формулы и\или принципы их работы.
1. Оцените её на отложенной выборке.
Вычислите accuracy (общую точность), точность (precision), recall (полноту), меру f1. Опишите эти показатели. 
1. Поэкспериментируйте с процедурой обработки данных. Удаляйте лишние столбцы, модифицируйте колонки, удаляйте выбросы и т.п. Обучайте модель после каждого эксперимента. Записывайте как каждый этап влияет на качество модели.
1. Если признаки имеют примерно одинаковую шкалу (минимумы и максимумы признаков отличаются на несколько процентов, или стандартные отклонения и средние значения признаков отличаются на несколько процентов), то опишите важность признаков, на основе коэффициентов регрессии.

Задание будет дополнено

# Лекция 7. Уменьшение размерности. Кластеризация.
05 апреля

- Уменьшение размерности, plotly, gradio: https://colab.research.google.com/drive/1SzAlYDLpjf65nnRbTVRtFxUWuyJnA8_s?usp=sharing
- Кластеризация: [учебное пособие](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf#page=111)
   - пример: [colab.research.google.com/drive/1OjrSLj2hWx-0AqZtTc4ydOH5d22DFDxf](https://colab.research.google.com/drive/1OjrSLj2hWx-0AqZtTc4ydOH5d22DFDxf)
 
### Задание 6. Кластеризация
https://github.com/ivtipm/ML/blob/main/tasks/clustering.md


# Лекция 8. Анасамбли моделей
12 апреля
- [colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg](https://colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg?usp=sharing)


#### Домашнее задание 8. Деревья. Ансамбли.
Примените методы в работах по классификации и регрессии, используйте поиск по сетке гиперпараметров:
- Решающее дерево
- Случайный лес
    - Оценить важность признаков; сравнить с результатами из матрицы корреляции.
- Бэггинг на основе решающих деревьев
- Комитет (простое или взвешенное голосование)
- Градиентный бустинг
- Комментируйте код, дополняйте формулами и схемами
- Создайте таблицу для сравнения качества моделей.
- Дополните выводы


# Лекция 9. Обработка текстовых данные
19 апреля
- Разведочный анализ и представление текста. Токенизация. Стемминг. Лемматизация. Стоп-слова. Частоты слов. Облака слов.  
- Представление текста: мешок слов, tf-idf, нейросетевые методы получения представлений текста
- Векторные представления слов. Word2vec, Glove и другие.
- Классификация текстов. Анализ тональности текстов.
- Слайды: https://docs.google.com/presentation/d/1o1TN-hI9BhVakKm4xI_S9ZS-lGV68iBUt6h3cX-DWQg/edit#slide=id.p 
- Пример:  https://colab.research.google.com/drive/1yOnvYUbbu7b2sgnh4vn1csis9PWAss_f

- Отслеживание экспериментов: https://github.com/ivtipm/ML/blob/main/wandb.md

upd: Примените нейросетевую модуль на основе архитектуры transformer для создания векторных представлений текстов.
Используйте векторные предсталвения тектов для классификации.\
Пример: https://colab.research.google.com/drive/1Aacg8tUXXNICQ0SetvqoIPD7GXnm0Ymx#scrollTo=e94a2272-af5a-4a17-a253-3fbcededbeb6



#### (Домашнее задание 9. Классификация текстов)[https://github.com/ivtipm/ML/blob/main/ML/text_classification.md]


# Лекция 10. Многослойный персептрон.
26 апреля
- Модель нейрона. Вход. Веса. Линейная регрессия в составе нейрона. Логиcтическая функция в составе нейрона. Функция активации (cигмода, ReLU).
- Многослойный персептрон (полносвязная нейросеть сеть). Соединения слоёв. Параметры нейросети. Построение нейросети в Keras, обучение и проверка. Особенности выходных слоёв для задачи классификации.  
- Слайды: https://docs.google.com/presentation/d/1YCJhQIj2BV42sDLKtxmytcYW5W39EJceYfqrYT7bKNo/edit#slide=id.p
- Пример: https://colab.research.google.com/drive/1YtK4an7UAhnxTmhmQzZd6Eo3esfv6TL3?usp=sharing#scrollTo=hCiu6Jn2d8JE&uniqifier=1


#### Домашнее задание 10. Многослойный персепторн. Keras
Добавьте в работу из задания 5 (классификация на реальных данных) нейросетевую модель на основе пакета Keras
Поэкспериментируйте. Подберите гиперпараметры модели (количество слоёв, количество нейронов в слое, функции активации, количество эпох обучения, шаг обучения).

Проверяйте модель по ходу обучения на валидационной выборке. Проверьте в конце на тестовой выборке

Используйте WanDB для отслеживания процесса и результатов обучения.


# Лекция 11. Многослойный персептрон (продолжение)
3 мая
- Обучение персептрона. Функции потерь. Метод обратного распространения ошибки для вычисления производных. 
- Метод градиентного спуска. Стохастический градиентный спуск. Learning rate (шаг обучения). Батчи. Эпохи.
- Обучение и переобучение 
- Слайды: https://docs.google.com/presentation/d/1YCJhQIj2BV42sDLKtxmytcYW5W39EJceYfqrYT7bKNo/edit#slide=id.p
- Пример: https://colab.research.google.com/drive/1YtK4an7UAhnxTmhmQzZd6Eo3esfv6TL3?usp=sharing#scrollTo=hCiu6Jn2d8JE&uniqifier=1


#### Домашнее задание 11. Многослойный персепторн. PyTorch
Добавьте в работу из задания 5 (классификация на реальных данных) нейросетевую модель на основе пакета PyTorch в двух вариантах: на основе класса Sequentil и на основе класса 
nn.Module.
Поэкспериментируйте. Подберите гиперпараметры модели (количество слоёв, количество нейронов в слое, функции активации, количество эпох обучения, шаг обучения).

Проверяйте модель по ходу обучения на валидационной выборке. Проверьте в конце на тестовой выборке

Используйте WanDB для отслеживания процесса и результатов обучения.

#### Домашнее задание 12. Шпаргалка по нейросетям
Сделайте шпаргалку по нейросетям.
    - Как зависит количество нейронов на выходном слое нейросети от:
        - решаемой задачи (классификация или регрессия)?
        - количества классов, если решается задача классификации?
    - Какие функции потерь стоит использовать для классификации, а какие для регрессии? Какие функции активации стоит использовать в выходных слоях нейросети в зависимости от задачи (классификация или регрессия)?
    - Какие показатели качества стоит использовать для оценки качества решения задачи регрессии и классификации?
    - Что показывает функция потерь?
    - Как выглядит переобучение на графике функции потерь? Как выглядит переобучение на графике показателя точности предсказания?
- Как бороться с переобучением?

* Шпаргалку можно привести в ipynb файле. Используйте заголовки, схемы и формулы.

upd: Дополните материалами для пакета Keras, PyTorch
    - Как представляются слои? Как их соединить?
    - Как задать функцию активации слоя?
    - Как задать функцию потерб для нейросети?
    - Как задачть объект\алгоритм минимизации функции ошибок?
    - Как задать lerning rate?




# Лекция 12. Многослойный персептрон (продолжение)
10 мая
- Ранняя остановка. 
- Представление слоёв в PyTorch. 
- Создание модели НС на основе nn.Module. Автоматический перебор архитектур.
- Dropout. BatchNormalisation
- Слайды: https://docs.google.com/presentation/d/1YCJhQIj2BV42sDLKtxmytcYW5W39EJceYfqrYT7bKNo/edit#slide=id.p
- Пример: https://colab.research.google.com/drive/1YtK4an7UAhnxTmhmQzZd6Eo3esfv6TL3?usp=sharing#scrollTo=hCiu6Jn2d8JE&uniqifier=1
- Векторные представления текстов c помощью моделей, основанных на архитектуре transformer.



# Лекция 13. Отбор признаков. Деплой моделей
17 мая
- [Отбор и создание признаков](../examples/features.ipynb)
- [Gradio](../examples/gradio.ipynb)
- Создание docker контейнеров

# Занятие 14. Собеседование
24 мая


# Экзамен
Консультация. суббота 8 июня (?), онлайн\
Вторник, 11 июня 14:00 оффлайн, ауд. 404\

Билет:
1. Теория.
2. Собеседование по одной из выполненных работ
3. Собеседование по одной из выполненных работ

Будьте готовы модифицировать и дополнить код.

## Примерный список тем и понятий
1. Понятие машинного обучения. Задачи. Типы алгоритмов. Регрессия vs классификация vs кластеризация. Разложение ошибки модели.
2. Метрики качества задач регрессии и классификации. Общая точность, точность, полнота, F beta, ROC AUC, R2, MSE, MAE, ...
3. Теория вероятностей и мат. статистика.
  - Вероятность. Закон больших чисел. Условная вероятность. Сложение и умножение событий. Теорема Байеса.
- Распределение. Числовые характеристики. Распределения: нормальное, биномиальное
- Выборка и генеральная совокупность. Бутстрэп.
  - Диаграмма размаха. Статистическая гипотеза. Гипотеза о равенстве (сравнении) средних. Гипотеза о соответствии распределению.  
  - Корреляция. КК Пирсона. Регрессия. Гипотеза о значимости КК Пирсона.
4. Анализ и подготовка данных. CRISP-DM.
  - Общий алгоритм анализа данных. Признаки, их виды. Пропуски. Выбросы. Ошибки.
  - Визуализация. Гистограмма. Диаграмма рассеяния. Тепловая карта. Диаграмма размаха.
  - Способы определения выбросов. Способы борьбы с пропусками, ошибками и выбросами.
  - Виды признаков. Способы кодирования с учётом и без учёта отношения порядка.
  - Модификация признаков. Приведение к нормальному распределению. Создание новых признаков. Отбор признаков.
  - Балансировка классов.  Андерсемплинг. Оверсемплинг. ADASYN. Tomek Links.
  - Нормализация и стандартизация.
  - Оценка качества модели. Переобучение и недообучение. Перекрёстная проверка.
  - Поиск по сетке гиперпараметров.
  - Примеры кода.
4. Метод наименьших квадратов.
5. Понятие методы оптимизации. Классификация. Метод стохастического градиентного спуска. Градиент. Производная сложной функции.\
Методы машинного обучения: параметры и гиперпараметры. Функции потерь. Преимущества и недостатки. Применение в задачах классификации и регрессии. Применение, примеры кода.
6. Линейная регрессия. Аналитическое решение. Регуляризации.
7. Метод опорных векторов. Модификации.
8. K ближайших соседей. Модификации.
9. Наивный байесовский классификатор.
10. Метод максимального правдоподобия. Вероятностная модель.
11. Логистическая регрессия.
12. Дерево решений.
13. Ансамбли. Голосование. Беггинг. Стэкинг. Блендинг. Адаптивный и градиентный бустинг.
14. Случайный лес.
16. Нейросеть (многослойный персептрон). Бинарная и многоклассовая классификация. Регрессия. Функции активации. Функции потерь. Обучение. Метод обратного распространения ошибки. Контроль переобучения. Раняя остановка.
17. Обучение без учителя. Кластеризация. Классификация алгоритмов. Показатели качества кластеризации. Метод kMeans. *Mean Shift. DBSCAN.*
18. Снижение размерности. PCA. UMAP. *SVD*
19. Обработка текста. Способы представления. Мешок слов. TF-IDF. Классификация.
20. Python. DataFrame. Nampy Array. Torch Tensor. Matplotlib. Plotly. Gradio. Конвейеры sklearn. Утилиты Linux для работы для получения сведений о аппаратном обеспечении, работы с файлами, простейшей обработки и анализа файлов. Google Colaboratory. Kaggle Notebook. Нейросети в Keras. Нейросети в pytorch.\ *wandb: Отслеживание показателей во время обучения, финальные измерения, диаграммы*\
Темы для дальнейшего самостоятельного изучения.
20. *Распределённые алгоритмы.* 
    Некоторые фреимворки для распределённых вычислений: PySpark (со своей ML библиотекой, dask)
21. *Рекомендательные системы*\
Бонус:
22. Портфолио участника соревнований по машинному обучению и примеры решения задач машинного обучения.


Подготовка конспекта-шпаргалки, интеллект-карты приветствуется.

Курсивом отмечены темы рекомендованные для дальнейшего изучения.
