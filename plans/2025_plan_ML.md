# О курсе
Курс посвящён машинному обучению - направлению искусственного интеллекта, где задачи решаются на основе анализа данных. Нередко большого количества данных. 
В курсе будут рассмотрены основные понятия машинного обучения, способы анализа и визуализации данных, преобразования данных в вид, подходящий для моделей машинного обучения. 
Будут рассмотрены классические модели машинного обучения, начиная от регрессий, заканчивая градиентным бустингом и случайным лесом (самые мощные модели, конкурирующие с нейросетями); задачи кластеризации; методы обучения и оптимизации качества этих моделей. В конце курса будут рассмотрены полносвязные нейросети (многослойный персептрон).

Задания будут посвящены преимущественно обработке табличных и текстовых данных на примере синтетических и реальных наборов данных.
Например: задача предсказания оттока клиентов банка (задача классификации), задача разделения покупателей мола по группам (кластеризация), задачи регрессии.

Рассматриваемые не нейросетевые методы МО применяются на практике, не смотря на большую популярность нейросетей. Курс можно считать как самостоятельным, так и длинным, но во многом обязательным, введением в курс нейросетевого анализа данных. Последний будет проходить в следующем семестре.

**См. также** план предыдущего курса, ссылки и примеры на главной странице, записи лекций предыдущего курса в плейлисте на Ютубе и в ТГ канале.

## Требования для успешного усвоения курса.

Для успешного освоения курса необходимы знания языка программирования Python, основ линейной алгебры, математического анализа, методов оптимизации,теории вероятностей и математической статистики.

Участие в соревнования по машинному обучению (см. например платформу kaggle) приветствуется и может быть зачтено вместо некоторых домашних заданий.


### Материалы для ознакомления перед началом курса
0. Шпаргалка по Python https://miro.com/app/board/uXjVNQC1rq8=/?share_link_id=860218532001
0. Теория вероятностей и математическая статистика
    - [raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_1.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_1.pdf)
   - [raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf)
   - [raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf)
   - Примеры распределений и их вид в зависимости от параметров: [seeing-theory.brown.edu/probability-distributions/index.html#section3](https://seeing-theory.brown.edu/probability-distributions/index.html#section3)
   - Среднее значение (average) и дисперсия (variance), интерактивный пример: [seeing-theory.brown.edu/basic-probability/index.html#section3](https://seeing-theory.brown.edu/basic-probability/index.html#section3)

0. Математическая статистика, основы numpy, pandas, matplotlib
    1. [Математические модели и вычислительные методы обработки экспериментальных данных](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf)
    1. 6 способов значительно ускорить pandas с помощью пары строк кода: https://habr.com/ru/articles/503726/, https://habr.com/ru/articles/504006/


# План
# 1. Обзор курса. Повторение Python. Venv. Jupyer Notebook
05 февраля
1. Обзор курса. 
1. Введение в Pyhton. 
    - Общая характеристика языка. Скорость выполнения программ. Синтаксис.
    - PIP3. Установка библиотек. Requirements.txt
    - Virtual Enviroment. Создание. Активация.
    - Jupyter Notebook: запуск в VS Code и в Google Colab.
        - Код и markdown. 
        - Выполнение программ с командным интерфейсом. 


### Задание 0. Python, NumPy и основные понятия математической статистики

Цель работы - освежить навыки использования Python, изучить отличия использования списков от массивов numpy array, использовать векторные операции.

Не используйте циклы, генераторы, функции стандартной библиотеки Питона sum, zip и map.

Используйте Google Colaboratory или другие среду подобную Jupyter Notebook.

Реализуйте функции двумя способами: 
- через циклы и списки Python;
- без использования циклов, через векторные вычисления и массивы numpy.

1. Для функций ошибок и величин, широко используемых в анализе данных и машинном обучении, напишите функции:
    1. для вычисления стандартного отклонения случайной величины;
    1. для вычисления ошибок MSE, MAE, accuracy, precision, recall, f1 score. 
    Функции должны принимать два параметра y_pred, y_true. См. аналогичные функции из библиотеки sklearn;
    1. для вычисления функции softmax на нескольких переменных, используйте `numpy.exp`;
    1. вычисления линейного коэффициента корреляции;
    1. для вычисления предсказания для нескольких объектов по заданному коэффициентами уравнения линейной регрессии (выполняйте только через numpy);
    1. подбора коэффициентов линейной регрессии, с использованием метода наименьших квадратов и функцией `minimize` модуля `scipy.optimize` (выполняйте только через numpy и scipy).
1. Напишите тесты для этих функций;
1. Пишите поясняющие и документирующие комментарии; Укажите зечем приведённые формулы используются.
1. Напишите формулы в формате LaTeX для всех функций, пояснения к формулам.

Для проверки кода, связанного с регрессией можно воспользоваться функцией `make_regression` из библиотеки Sklearn. Она создаёт синтетические данные для задачи регрессии. 

См. шпаргалку по Python: https://miro.com/app/board/uXjVNQC1rq8=, учебное пособие [Математические модели и вычислительные методы обработки экспериментальных данных](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf)

Шпаргалка по LaTex: https://pelican.study/latex

Задание может быть дополнено.



# Лекция 2.  Введение в машинное обучение. Математическая статистика. NumPy. Pandas. Синтетические данные. EVA. Линейная регрессия.
12 февраля
1. Введение: [docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing)
1. Теория вероятностей и математическая статистика:
    - [raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/variables_2.pdf)
    - [raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf](https://raw.githubusercontent.com/VetrovSV/AppMathST/master/statistics.pdf)

0. учебное пособие [Математические модели и вычислительные методы обработки экспериментальных данных](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf) - о построение графиков, numpy, pandas.

1. https://colab.research.google.com/drive/18YGaumubomt-Rtg_9_VT_nf1GwbMlEx8 - синтетические данные. EVA. Линейная регрессия.

2. [EDA, предобработка, кодирование colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR](https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR)

2. [Линейная регрессия. МНК. Метод макс.правдоподобия. sklearn.ipynb colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx](https://colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx?usp=sharing)


### Задание 1. Основы Google colab. Синтетические данные. Разведочное исследование данных (EDA). Линейная регрессия.
Выполняйте пункты задания по мере изучения соответствующих тем на лекциях.

1. Освойтесь в google colaboratory
    - Как создавать ячейки? Какие виды ячеек бывают?
    - Какие способы запуска ячеек есть? Как ячейки с кодом влияют друг на друга?
    - Что такое markdown? Как создавать заголовки (и просматривать их в Google Colaboratory), делать текст жирным, курсивом, приводить фрагменты кода с подсветкой синтаксиса, приводить формулы в записи LaTeX?
    - Как делиться проектом (тетрадкой - notebook)?
    - Приведите ссылку на задание, текст задания.
1. Изучите пример 1 из лекции. 
1. Создайте синтетический набор данных для решения задачи регрессии. Количество объектов должно быть меньше 10к, количество независимых признаков - 4 или больше, среди них 1 или 2 не должны влиять на  целевую переменную. 
1. Разделите исходные данные на выборку для обучения (train) и отложенную выборку (test).
1. Исследуйте данные. 
    - Изучите числовые характеристики всех признаков через 7-point summary. Используйте метод `describe()` класса pandas.DataFrame. Напишите пояснения для этих характеристик.
    - Постройте диаграммы размаха для всех признаков (seaborn.boxplot), скрипичные диаграммы (violinplot). Напишите пояснения. Есть ли выбросы?
    - Разберитесь как задать размер полотна (изображения) для графиков? Как построить несколько графиков, на одном полотне, но с разными осями? Постройте несколько диаграмм размаха 
    - Постройте гистограмму для целевого признака. Используйте рекомендованное библиотекой число столбцов, задайте много столбцов. Опишите, как такая гистограмма устроена. Видны ли на гистограмме с большим количеством столбцов аномалии в данных?
    - Вычислите матрицу корреляции, сделайте для неё тепловую карту. Напишите, какие независимые признаки влияют на целевой сильнее всего? Есть ли математическая зависимость между независимыми признаками? Как это влияет на качество уравнения линейной регрессии?
    - Постройте попарные дигаммы рассеяния (seabor.pairplot). Что показывает такая диаграмма? Для какого распределения целевого признака предсказания уравнения линейной регрессии будут точнее всего? Постройте для этого признака отдельную диаграмму рассеяния (seaborn.jointplot).
    - Дополнительно используйте пакет plotly для построения двухмерной и трёхмерной диаграммы рассеяния. На диаграмме должен быть целевой признак, и два наиболее значимых независимых признака.
    - Бонус: дополнительно постройте диаграммы на свой выбор. 
1. Постройте модель линейной регрессии. Используйте sklearn.
1. Оцените качество модели на трейне и на тесте. Почему результаты могут отличаться?
    - Вычисляйте ошибку MSE, MAE.  Используйте встроенные в sklearn функции. Используйте функции, созданные в предыдущем задании.
    - Вычислите коэффициент детерминации R2. О чём он говорит?  
    - приведите функцию потерь, пояснения к ней. Зачем нужна эта функция?    
    - upd: сравните графики распределения предсказанных значениий и целевого признака из тестового датасета.
3. Напишите пояснения и комментарии к коду. Поясняйте общий алгоритм, смысл действий, понятия (std, mean, ...) параметры вызываемых функций, записанные в коде формулы (приведите их в LaTeX). Комментарии к коду можно оставлять в ячейках с кодом, остальные пояснения можно давать в ячейках с текстом.


# Лекция 2.  Введение в машинное обучение. Pandas. Синтетические данные. Линейная регрессия. Показатели качества предсказания.
12 февраля
1. Введение: [docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing)
    - Понятие машинного обучения
    - Обучение с подкреплением, с учителем, без учителя
    - Задача предсказания: регрессия и классификация
    - Создание синтетического датасета в виде табличных данных, пакет sklearn
    - Табличные данные, pandas.DataFrame, массивы numpy array, индексация и срезы.
    - API для построения моделей в пакете sklearn: fit, predict; Пример построения модели линейной регрессии.
    - Показатели качества предсказания
        - Регрессия: MAE (средний модуль ошибки), MSE (средний квадрат ошибки), ... 
        - Классификация: Accuracy (общая точность), Precision (точность), Recall (полнота), F1

0. учебное пособие [Математические модели и вычислительные методы обработки экспериментальных данных](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf) - о построение графиков, numpy, pandas.

1. https://colab.research.google.com/drive/18YGaumubomt-Rtg_9_VT_nf1GwbMlEx8 - синтетические данные. EDA. Линейная регрессия.
1. https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR - EDA. Подготовка данных.

2. [Линейная регрессия. МНК. Метод макс.правдоподобия. sklearn.ipynb colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx](https://colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx)


# Лекция 3. Линейная регрессия (продолжение). Разведочный анализ данных (EDA)
19 февраля
0. Повторение. Показатели качества модели регрессии и классификации. 
1. Подбор параметров уравнения линейной регрессии. Метод наименьших квадратов. Понятие функции потерь. Понятие градиентного спуска. Аналитическое решение для коэффициентов линейной регрессии
 [Линейная регрессия. МНК. Метод макс.правдоподобия. sklearn.ipynb colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx](https://colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx)
1. Оценка качества задачи регрессии. Тестовая и обучающая выборки. Переобучение и недообучение. 


# Лекция 4. Метод градиентного спуска. Разведочный анализ данных (EDA)
19 февраля
0. Повторение.
1. Метод градиентного спуска.
2. Разведочный анализ данных (Exploratory data analysis, EDA):
    - технический анализ: количество данных, количество признаков, типы данных
    - пропуски, выбросы, ошибки и аномалии 
    - 7 point summary
    - boxplot, violin plot
    - диаграмма пропусков
    - гистограмма (распределение)
    - корреляции и влияние независимых признаков на целевой
    - диаграмма рассеяния
    - См. учебное пособие [Математические модели и вычислительные методы обработки экспериментальных данных
1. Подбор параметров уравнения линейной регрессии. Метод наименьших квадратов. Понятие функции потерь. Понятие градиентного спуска. Аналитическое решение для коэффициентов линейной регрессии
 [Линейная регрессия. МНК. Метод макс.правдоподобия. sklearn.ipynb colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx](https://colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx)
1. Оценка качества задачи регрессии. Тестовая и обучающая выборки. Переобучение и недообучение. 



# Лекция 4. EDA (продолжение). Подготовка данных. Задача классификации. Логистическая регрессия. Показатели качества классификации.
5 марта
- Повторение
- Подготовка данных
    - Числовые и нечисловые признаки. Числовое кодирование. Унитарный код (one-hot кодирование) 
    - Пропуски.  Дубликаты.
    - Выбросы.
    - Масштабирование.
- Линейная разделимость классов. Сигмоида. Вероятностная модель; LogLoss.
- [colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh?usp=sharing](https://colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh?usp=sharing)
- Показатели качества классификации: [docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM/edit?usp=sharing)
    - Матрица ошибок
    - Accuracy (общая точность), Precision (точность), Recall (полнота), F1
    - ROC AUC (для бинарной классификации)


# Домашнее задание 2. Классификация на синтетических данных
0. Приведите ссылку на задание, текст задания.
1. Сгенерируйте данные. Создайте 5 или больше независимых признака, в том числе 1-2 лишних, 2 класса. [ [пример](https://colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh#scrollTo=57PKhQ2Ylni1) ]. Создавайте не более 10 тысяч объектов.
1. Разделите данные на трейн и тест.
1. Исследуйте данные. 
    - Изучите числовые характеристики всех признаков через 7-point summary. Используйте метод `describe()` класса pandas.DataFrame. Напишите пояснения для этих характеристик.
    - Проверьте баланс классов. Что такое плохой баланс классов? Как он может повлиять на качество модели?
    - Постройте диаграммы размаха для всех признаков (seaborn.boxplot), скрипичные диаграммы (violinplot). Напишите пояснения. Есть ли выбросы?
    - Разберитесь как задать размер полотна (изображения) для графиков? Как построить несколько графиков, на одном полотне, но с разными осями? Постройте несколько диаграмм размаха 
    - Имеет ли здесь смысл построение гистограммы распределения целевого признака?
    - Вычислите матрицу корреляции, сделайте для неё тепловую карту. Напишите, какие независимые признаки влияют на целевой сильнее всего? Есть ли математическая зависимость между независимыми признаками? Как это влияет на качество уравнения логистической регрессии? Имеет ли смысл считать коэф. корреляции для категориального признака?
    - Постройте попарные дигаммы рассеяния (seabor.pairplot). Кодируйте класс цветом. Что показывает такая диаграмма? Постройте для этого признака отдельную диаграмму рассеяния (seaborn.jointplot).
    - Дополнительно используйте пакет plotly для построения двухмерной и трёхмерной диаграммы рассеяния. На диаграмме целевой признак должен быть отмечен цветом.
    - Бонус: дополнительно постройте диаграммы на свой выбор. 
1. Обучите модель логистической регрессии. Приведите общую формулу, пояснения для неё. Приведите функцию потерь, пояснения к ней. Зачем она нужна?
1. Оцените её на отложенной выборке. Вычислите 
    - accuracy (общую точность),
    - точность (precision), 
    - recall (полноту), 
    - меру f1.
    - используйте функцию `classification_report`, приведите пояснения для таблицы.
    - ROC AUC
    - Опишите эти показатели. 
    - upd: сравните графики распределения предсказанных значениий и целевого признака из тестового датасета.
3. Напишите пояснения и комментарии к коду. Поясняйте общий алгоритм, смысл действий, понятия (std, mean, ...) параметры вызываемых функций, записанные в коде формулы (приведите их в LaTeX). Комментарии к коду можно оставлять в ячейках с кодом, остальные пояснения можно давать в ячейках с текстом.



# Домашнее задание 3. EVA и классификация на реальных данных
0. Приведите ссылку на задание, текст задания.
1. Используйте один из предложенных датасетов:
    - вариант 1. https://raw.githubusercontent.com/ivtipm/ML/main/datasets/churn.csv (есть на Kaggle)
    - вариант 2 (сложнее, но даёт примерно в 1.5 раза больше баллов). https://archive.ics.uci.edu/ml/datasets/adult, https://www.kaggle.com/wenruliu/adult-income-dataset?select=adult.csv
    - можно обработать оба
или предложите свой датасет, сопоставимый по количеству и качеству данных с рекомендованным. Используйте свой датасет после согласования с преподавателем.
1. Проведите разведочный анализ данных.
    - 7 point summary,
    - Пропуски, дубликаты
    - диаграммы размаха, скрипичные диаграммы (violin plot),
    - матрица корреляции,
    - попарные диаграммы рассеяния,
    - столбчатую диаграмму для целевого признака, что можно сказать про баланс классов?
    - Коротко опишите данные. Сколько признаков? Какие признаки сильнее всего влияют на целевой признак? Есть ли выбросы? Какие распределения имеют признаки (нормальное, равномерное и т.п.)?
    - Удалите лишние признаки, кодируйте нечисловые признаки, удалите выбросы, масштабируйте признаки. Поясняйте действия, описывайте зачем их производите.
    - upd: Бонус: интерактивные диаграммы на основе plotly: гистограммы для важны?
    - Опишите данные: какие данные даны? Сколько признаков и объектов? Есть ли пропуски, дубликаты, выбросы
    -  *В начале работы рекомендуется удалить все неудобные (много пропусков, нужно кодировать, есть ошибки и т.п.) признаки и построить простую модель. Записывайте как проводили обработку данных и результаты. Далее, можно более тщательно изучить и подготовить данные, снова обучить модель и сравнить результаты с предыдущим разом.*
1. Обучите модели:
    - Логистическую регрессию
    - SVM
    - kNN
    - Решающее дерево
    - др. модели, которые будут рассмотренны на следующих занятиях
1. Подберите гиперпараметры моделей.
1. Опишите модели, формулы и\или принципы их работы.
1. Оцените её на отложенной выборке.
Вычислите accuracy (общую точность), точность (precision), recall (полноту), меру f1. Опишите эти показатели. 
1. Поэкспериментируйте с процедурой обработки данных. Удаляйте лишние столбцы, модифицируйте колонки, удаляйте выбросы и т.п. Обучайте модель после каждого эксперимента. Записывайте как каждый этап влияет на качество модели.
1. Если признаки имеют примерно одинаковую шкалу (минимумы и максимумы признаков отличаются на несколько процентов, или стандартные отклонения и средние значения признаков отличаются на несколько процентов), то опишите важность признаков, на основе коэффициентов регрессии.

Задание будет дополнено


# Лекция 6. kNN. SVM.
15 марта
- k-ближайших соседей (k nearest neighbors). Гиперпараметры. Модификации kNN.
- Метод опорных векторов (support vector machine). Гиперпараметры.
- https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR?usp=sharing - EVA
- https://colab.research.google.com/drive/1oh7-ID00MN-AoJtAm4uL-bj0sqf5ieuk - kNN
- Перекрёстная проверка (cross validation)


### Домашнее задание 3.
3. Используйте метод кросс-валидации для обучения и оценки модели на небольшой выборке.


### Домашнее задание 4. 
1. Добавьте в работы 1 и 2 модели:
    - KNN
    - SVM
    - Решающее дерево

2. Подберите гиперпараметры этих моделей. Используйте класс GridSearchCV.



# Домашнее задание: Конспект по моделям МО
1. Кратко опишите модели МО, которые рассматриваются на этом курсе. Опишите модель, её обучение, функцию потерь (если есть), особенности, важные гиперпараметры, преимущества и недостатки. См. задание про конспект о нейросетях (персептронах) ниже.

1. Что такое вероятностная модель? Что такое линейная модель?

1. Что такое поиск по сетке гиперпараметров? Зачем он нужен. Как определить количество комбинаций гиперпараметров? Приведите пример для одной из моделей sklearn.

1. Что такое кросс-валидация? Когда она нужна? Приведите пример.

1. Какие показатели качества решения задачи предсказания (классификации и регрессии) существуют? В каких случах их нужно и не нужно использовать?

Не храните конспекты в разрозненных файлах. 
Рекомендуется выполнять конспект в гугл-документах, Obsidian, notion или подобных программах.


# Домашнее задание: КОнспект по обработке и анализу данных.
1. Что такое объект, признак? Какие категории признаков бывают? Что такое независимые и целевые признаки?
1. Опишите параметры из 7-point summary
1. Опишите основные виды диаграмм из заданий. Зачем их создавать?
1. Как можно кодировать категориальные признаки? В какой случае какой вид кодирования использовать?
1. Зачем масштабировать признаки? Нужно ли масштабировать целевой признак? Для каких моделей масштабирование не имеет смысла?


# Лекция 6. Поиск по сетке гиперпараметров. Решающие деревья. CRISP-DM.
22 марта\
0. Повторение
1. Поиск по сетке гиперпараметров. [[Теория и пример](https://colab.research.google.com/drive/1oh7-ID00MN-AoJtAm4uL-bj0sqf5ieuk?usp=sharing)]
2. Решающие деревья [[colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg#scrollTo=o3dx6WQiX1m7](https://colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg#scrollTo=o3dx6WQiX1m7)]. Обучение. Гиперпараметры. Решение задачи классификации и регрессии. Поиск выбросов. 
3. EVA. CRISP-DM. Разбор примера. [ [https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR?usp=sharing#scrollTo=b4n53izzoxC3] ]
    - начальный анализ
    - пропуски и неизвестные значения, выбросы, аномалии
    - кодирование нечисловых признаков: one_hot, label encoding, ручное кодирование по словарю
    - масштабирование (нормирование и стандартизация)




# Лекция 7. Уменьшение размерности. Кластеризация.
05 апреля

- Уменьшение размерности, plotly, gradio: https://colab.research.google.com/drive/1SzAlYDLpjf65nnRbTVRtFxUWuyJnA8_s?usp=sharing
- Кластеризация: [учебное пособие](https://raw.githubusercontent.com/ivtipm/ML/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%B8%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf#page=111)
   - пример: [colab.research.google.com/drive/1OjrSLj2hWx-0AqZtTc4ydOH5d22DFDxf](https://colab.research.google.com/drive/1OjrSLj2hWx-0AqZtTc4ydOH5d22DFDxf)
 
# [Задание 6. Кластеризация](https://github.com/ivtipm/ML/blob/main/tasks/clustering.md)



# Лекция. Обработка текстовых данных
30 апреля
- Разведочный анализ и представление текста. Токенизация. Стемминг. Лемматизация. Стоп-слова. Частоты слов. Облака слов.  
- Представление текста: мешок слов, tf-idf, нейросетевые методы получения представлений текста
- Векторные представления слов. Word2vec, Glove и другие. Векторные представления из пакет sentence transformer;
- Классификация текстов. Анализ тональности текстов.
- Слайды: https://docs.google.com/presentation/d/1o1TN-hI9BhVakKm4xI_S9ZS-lGV68iBUt6h3cX-DWQg/edit#slide=id.p 
- Пример:  https://colab.research.google.com/drive/1yOnvYUbbu7b2sgnh4vn1csis9PWAss_f

- Отслеживание экспериментов: https://github.com/ivtipm/ML/blob/main/wandb.md

upd: Примените нейросетевую модуль на основе архитектуры transformer для создания векторных представлений текстов.
Используйте векторные представления текстов для классификации.\
Пример: https://colab.research.google.com/drive/1XDLKd42U5UcDSfwXtDgR3uJuFND9xCvS







# Лекция. Ансамбли моделей
7 мая
- Случайный лес.
- Комитет
- Бэггинг
- Адаптивный и градиентный бустинг.
- [colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg](https://colab.research.google.com/drive/1Bin_h7BPSfnxs4Pea7eibceNMkOEcKmg?usp=sharing)


# Домашнее задание. Деревья. Ансамбли.
Примените методы в работах по классификации и регрессии, используйте поиск по сетке гиперпараметров:
- Решающее дерево
- Случайный лес
    - Оценить важность признаков; сравнить с результатами из матрицы корреляции.
- Бэггинг на основе решающих деревьев
- Комитет (простое или взвешенное голосование)
- Градиентный бустинг
- Комментируйте код, дополняйте формулами и схемами
- Создайте таблицу для сравнения качества моделей.
- Дополните выводы


# Лекция. Отслеживание экспериментов. W&B. MLFlow
15 мая\
[tools/wandb_mlflow.md](tools/wandb_mlflow.md)



# [Домашнее задание. Классификация текстов](https://github.com/ivtipm/ML/blob/main/ML/text_classification.md)


# Домашнее задание. Отслеживание экспериментов и сравнение моделей (WB, MLFlow)
- Логируйте эксперименты \ варианты подготовки данных и обучения моделей через WB (Weights and Biases, WandB), MLflow или аналогичный сервис
- Постройте таблицы для сравнения: 
    - моделей и значений гиперпараметров
    - признаков, отличающихся: количеством, составом, способом кодирования, способом очистки, масштабированием и т.д.
- Напишите выводы
# Лекция. Многослойный персептрон.

- Модель нейрона. Вход. Веса. Линейная регрессия в составе нейрона. Логистическая функция в составе нейрона. Функция активации (cигмода, ReLU).
- Многослойный персептрон (полносвязная нейросеть сеть). Соединения слоёв. Параметры нейросети. Построение нейросети в Keras, обучение и проверка. Особенности выходных слоёв для задачи классификации.  
- Слайды: https://docs.google.com/presentation/d/1YCJhQIj2BV42sDLKtxmytcYW5W39EJceYfqrYT7bKNo/edit#slide=id.p
- Пример: https://colab.research.google.com/drive/1YtK4an7UAhnxTmhmQzZd6Eo3esfv6TL3?usp=sharing#scrollTo=hCiu6Jn2d8JE&uniqifier=1


# Домашнее задание. Многослойный персептрон. Keras
1. Добавьте в работы по регрессии и классификации нейросетевую модель
    - Реализованную на основе пакета Keras.
    - Реализованную на основе фреимворка PyTorch.
1. Поэкспериментируйте. Подберите гиперпараметры модели (количество слоёв, количество нейронов в слое, функции активации, количество эпох обучения, шаг обучения).

1. Проверяйте модель по ходу обучения на валидационной выборке. Проверьте в конце на тестовой выборке

1. Используйте WanDB или аналогичный сервис (например MLFLow) для отслеживания процесса и результатов обучения.


# Домашнее задание. Шпаргалка по нейросетям
Сделайте шпаргалку по нейросетям.
  - Как зависит количество нейронов на выходном слое нейросети от:
        - решаемой задачи (классификация или регрессия)?
        - количества классов, если решается задача классификации?
  - Какие функции потерь стоит использовать для классификации, а какие для регрессии? Какие функции активации стоит использовать в выходных слоях нейросети в зависимости от задачи (классификация или регрессия)?
  - Какие показатели качества стоит использовать для оценки качества решения задачи регрессии и классификации?
  - Что показывает функция потерь?
  - Как выглядит переобучение на графике функции потерь? Как выглядит переобучение на графике показателя точности предсказания?
- Как бороться с переобучением?

* Шпаргалку можно привести в ipynb файле. Используйте заголовки, схемы и формулы.


Дополните материалами для пакета Keras, PyTorch
- Как представляются слои? Как их соединить?
- Как задать функцию активации слоя?
- Как задать функцию потерь для нейросети?
- Как задать объект\алгоритм минимизации функции ошибок?
- Как задать learning rate?


# Лекция 11. Многослойный персептрон (продолжение)
- Обучение персептрона. Функции потерь. Метод обратного распространения ошибки для вычисления производных. 
- Метод градиентного спуска. Стохастический градиентный спуск. Learning rate (шаг обучения). Батчи. Эпохи.
- Обучение и переобучение 
- Слайды: https://docs.google.com/presentation/d/1YCJhQIj2BV42sDLKtxmytcYW5W39EJceYfqrYT7bKNo/edit#slide=id.p
- Пример: https://colab.research.google.com/drive/1YtK4an7UAhnxTmhmQzZd6Eo3esfv6TL3?usp=sharing#scrollTo=hCiu6Jn2d8JE&uniqifier=1








# Лекция 12. Многослойный персептрон (продолжение)
10 мая
- Ранняя остановка. 
- Представление слоёв в PyTorch. 
- Создание модели НС на основе nn.Module. Автоматический перебор архитектур.
- Dropout. BatchNormalization
- Слайды: https://docs.google.com/presentation/d/1YCJhQIj2BV42sDLKtxmytcYW5W39EJceYfqrYT7bKNo/edit#slide=id.p
- Пример: https://colab.research.google.com/drive/1YtK4an7UAhnxTmhmQzZd6Eo3esfv6TL3?usp=sharing#scrollTo=hCiu6Jn2d8JE&uniqifier=1
- Векторные представления текстов c помощью моделей, основанных на архитектуре transformer.



# Лекция 13. Отбор признаков. Деплой моделей

- [Отбор и создание признаков](../examples/features.ipynb)
- [Gradio](../examples/gradio.ipynb)
- Создание docker контейнеров

# Занятие 14. Собеседование
???


# Экзамен
Консультация: ...\
Вторник, 14 июня 16:00 оффлайн, ауд. 409


### Допуск на экзамен
1. Задачи
    - [ ] 📏 Решена любая задача регрессии :
    - [ ] 🆎 Решена любая задача классификации
    - [ ] 📊 Решена задача с обработкой реальных табличных данных 
    - [ ] 📃 Решена задача классификации текстов
1. 👨🏼‍🏫 Хотя бы один раз использованы модели:
    - [ ] Линейная регрессия
    - [ ] Логистическая регрессия
    - [ ] SVM
    - [ ] Дерево решений
    - [ ] Случайный лес
    - [ ] Градиентный бустинг
    - [ ] Многослойный персептрон
1. 📘📘 конспекты:
    - Вышеприведённые модели: общее описание, формулы, гиперпараметры, преимущества и недостатки
    - Анализ и обработка данных: диаграммы, статистические величины, кодирование, масштабирование, отбор признаков, ...
    - Персептрон. Обучение. Гиперпараметры.
1. 🔠 Для этих моделей выполнен поиск по сетке гиперпараметров
1. Отслеживание экспериментов (WB, MLFlow).
1. ???



### Билет:
1. Теория.
2. Собеседование по одной из выполненных работ
3. Собеседование по одной из выполненных работ

Будьте готовы модифицировать и дополнить код.

## Примерный список тем и понятий
1. Понятие машинного обучения. Задачи. Типы алгоритмов. Регрессия vs классификация vs кластеризация. Разложение ошибки модели.
2. Метрики качества задач регрессии и классификации. Общая точность, точность, полнота, F beta, ROC AUC, R2, MSE, MAE, ...
3. Теория вероятностей и мат. статистика.
    - Вероятность. Закон больших чисел. Условная вероятность. Сложение и умножение событий. Теорема Байеса.
    - Распределение. Числовые характеристики. Распределения: нормальное, биномиальное
    - Выборка и генеральная совокупность. Бутстрэп.
    - Диаграмма размаха. Статистическая гипотеза. Гипотеза о равенстве (сравнении) средних. Гипотеза о соответствии распределению.  
    - Корреляция. КК Пирсона. Регрессия. Гипотеза о значимости КК Пирсона.
4. Анализ и подготовка данных. CRISP-DM.
    - Общий алгоритм анализа данных. Признаки, их виды. Пропуски. Выбросы. Ошибки.
    - Визуализация. Гистограмма. Диаграмма рассеяния. Тепловая карта. Диаграмма размаха.
    - Способы определения выбросов. Способы борьбы с пропусками, ошибками и выбросами.
    - Виды признаков. Способы кодирования с учётом и без учёта отношения порядка.
    - Модификация признаков. Приведение к нормальному распределению. Создание новых признаков. Отбор признаков.
    - Балансировка классов.  Андерсемплинг. Оверсемплинг. ADASYN. Tomek Links.
    - Нормализация и стандартизация.
    - Оценка качества модели. Переобучение и недообучение. Перекрёстная проверка.
    - Поиск по сетке гиперпараметров.
    - Примеры кода.
4. Метод наименьших квадратов.
5. Понятие методы оптимизации. Классификация. Метод стохастического градиентного спуска. Градиент. Производная сложной функции.\
Методы машинного обучения: параметры и гиперпараметры. Функции потерь. Преимущества и недостатки. Применение в задачах классификации и регрессии. Применение, примеры кода.
6. Линейная регрессия. Аналитическое решение. Регуляризации.
7. Метод опорных векторов. Модификации.
8. K ближайших соседей. Модификации.
9. Наивный байесовский классификатор.
10. Метод максимального правдоподобия. Вероятностная модель.
11. Логистическая регрессия.
12. Дерево решений.
13. Ансамбли. Голосование. Беггинг. Стэкинг. Блендинг. Адаптивный и градиентный бустинг.
14. Случайный лес.
16. Нейросеть (многослойный персептрон). Бинарная и многоклассовая классификация. Регрессия. Функции активации. Функции потерь. Обучение. Метод обратного распространения ошибки. Контроль переобучения. Раняя остановка.
17. Обучение без учителя. Кластеризация. Классификация алгоритмов. Показатели качества кластеризации. Метод kMeans. *Mean Shift. DBSCAN.*
18. Снижение размерности. PCA. UMAP. *SVD*
19. Обработка текста. Способы представления. Мешок слов. TF-IDF. Классификация.
20. Python. DataFrame. Nampy Array. Torch Tensor. Matplotlib. Plotly. Gradio. Конвейеры sklearn. Утилиты Linux для работы для получения сведений о аппаратном обеспечении, работы с файлами, простейшей обработки и анализа файлов. Google Colaboratory. Kaggle Notebook. Нейросети в Keras. Нейросети в pytorch.\ *wandb: Отслеживание показателей во время обучения, финальные измерения, диаграммы*\
Темы для дальнейшего самостоятельного изучения.
20. *Распределённые алгоритмы.* 
    Некоторые фреимворки для распределённых вычислений: PySpark (со своей ML библиотекой, dask)
21. *Рекомендательные системы*\
Бонус:
22. Портфолио участника соревнований по машинному обучению и примеры решения задач машинного обучения.


Подготовка конспекта-шпаргалки, интеллект-карты приветствуется.

Курсивом отмечены темы рекомендованные для дальнейшего изучения.


