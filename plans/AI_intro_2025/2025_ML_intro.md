# Введение в разработку интеллектуальных систем

## После освоения курса Вы сможете
* Ориентироваться в основных понятиях машинного обучения (МО): виды МО, данные, модели, методы МО и т.д.

* Понимать жизненный цикл ML-проекта и выбирать подходящие этапы: сбор данных, анализ данных, подготовка данных, обучение модели, валидация модели, создание приложения на основе модели.

* Обучать и оценивать простые модели надёжно: линейная/логистическая регрессия; строить пайплайны и кросс-валидацию.

* Выбирать и интерпретировать ключевые метрики качества (MAE/MSE/R², Accuracy/Precision/Recall/F1) и диагностировать переобучение и недообучение.

* Сериализовать модель и подготовить минимальный интерфейс/API (Gradio/FastAPI) для демонстрации результата.

См. также курс по программированию для МО: https://github.com/VetrovSV/Programming/blob/master/plans/ML/readme.md

# План
1. Основы ИИ: определения ИИ, машинного обучения (МО) и их взаимосвязь; отличие от классического программирования; примеры применения. История ИИ. [слайды - 1](https://docs.google.com/presentation/d/11zFsnGJAaCzFGGVAjrd_OyNGD-fB6uLr3R-azaof0Qk)

2. Жизненный цикл ML-проекта: этапы от бизнес-задачи до развертывания;
    - CRISP-DM.  [слайды - 1](https://docs.google.com/presentation/d/11zFsnGJAaCzFGGVAjrd_OyNGD-fB6uLr3R-azaof0Qk)
    - Разведочный анализ данных. Пример: https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR
        - Пропуски.
        - Выбросы.
        - Запросы к DataFrame.
        - Категориальные и числовые признаки. One-hot кодирование.

3. Данные для МО: понятия признак (feature) и целевая переменная (target), типы данных, концепция разведочного анализа данных (EDA). [слайды - 1](https://docs.google.com/presentation/d/11zFsnGJAaCzFGGVAjrd_OyNGD-fB6uLr3R-azaof0Qk), [слайды - 2](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM)

4. Обучения с учителем: задачи регрессии и классификации; разделение данных на обучающую (train) и тестовую (test) выборки. [слайды - 1](https://docs.google.com/presentation/d/11zFsnGJAaCzFGGVAjrd_OyNGD-fB6uLr3R-azaof0Qk)

5. Простые модели: линейная и логистическая регрессия. Функции потерь: MSE, LogLoss. Использование scikit-learn: fit, predict.
    - Линейная регрессия: модель и пример: https://colab.research.google.com/drive/1YadlNYk9_WkCQY6L-HKP9SI7xZjuzIMx
    - [Короткая шпаргалка по линейной регрессии](https://github.com/ivtipm/ML/blob/main/slides%20etc/linreg/ML_intro_linreg.png?raw=true)
    - Логистическая регрессия: модель и пример:
        - https://colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh?usp=sharing#scrollTo=UmtvOZb6DzgM
        - https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR#scrollTo=bG_ytuO0xECz
    - *Пайплайны обработки данных и применение модели*

6. Качество модели: метрики для регрессии (MAE, MSE, R²), метрики для классификации (Accuracy, Precision, Recall, F1-score), матрица ошибок, визуальное объяснение недообучения (underfitting) и переобучения (overfitting). Воспроизводимость экспериментов. [слайды - 2](https://docs.google.com/presentation/d/1mK9CfhwjQtAdJZENV3vU4nCGSkzI8_Ugkv_AavBVEaM)

7. Другие парадигмы в ИИ (Обзор): обучение без учителя (Unsupervised Learning), кластеризация; обучение с подкреплением (Reinforcement Learning), агент, среда, вознаграждение; краткий обзор других направлений.

8. Развертывание и использование модели: сериализация модели, концепция API для модели, создание простого веб-приложения с помощью Gradio и FastAPI.
    - Коротко о RESTful API → OpenAPI → FastAPI: https://github.com/ivtipm/ML/tree/main/examples/FastAPI
    - Создание простого пользовательского интерфейса (Gradio): https://github.com/ivtipm/ML/blob/main/examples/gradio.ipynb
9. Простейшее приложение в докер-контейнере.\
   Примеры:
    - https://github.com/ivtipm/ML/blob/main/examples/docker-ML-web
    - https://github.com/ivtipm/ML/blob/main/examples/docker-api


**См. также** [план по Языкам программирования для МО](https://github.com/VetrovSV/Programming/blob/master/plans/ML/readme.md)

# Задания

## Линейная регрессия. Синтетические данные.

Используйте библиотеку sklearn.

1. Освойтесь в google colaboratory или VS Code
    - Как создавать ячейки? Какие виды ячеек бывают?
    - Какие способы запуска ячеек есть? Как ячейки с кодом влияют друг на друга?
    - Что такое markdown? Как создавать заголовки (и просматривать их в Google Colaboratory), делать текст жирным, курсивом, приводить фрагменты кода с подсветкой синтаксиса, приводить формулы в записи LaTeX?
    - Как делиться проектом (тетрадкой - notebook)?
    - Приведите ссылку на задание, текст задания.

1. Создайте синтетический набор данных для решения задачи линейной регрессии. Количество объектов должно быть меньше 500, количество независимых признаков - 4 или больше, среди них 1 или 2 не должны влиять на  целевую переменную. Используйте функцию make_regression из sklearn. Задавайте random_state равным номеру зачётной книжки.

1. Исследуйте данные.
    - Изучите числовые характеристики всех признаков через 7-point summary. Используйте метод `describe()` класса pandas.DataFrame. Напишите пояснения для этих характеристик.
    - Постройте диаграммы размаха для всех признаков (seaborn.boxplot), скрипичные диаграммы (violinplot). Напишите пояснения. Есть ли выбросы?
    - Разберитесь как задать размер полотна (изображения) для графиков? Как построить несколько графиков, на одном полотне, но с разными осями? Постройте несколько диаграмм размаха
    - Постройте гистограмму для целевого признака. Используйте рекомендованное библиотекой число столбцов, задайте много столбцов. Опишите, как такая гистограмма устроена. Видны ли на гистограмме с большим количеством столбцов аномалии в данных?
    - Вычислите матрицу корреляции, сделайте для неё тепловую карту. Напишите, какие независимые признаки влияют на целевой сильнее всего? Есть ли математическая зависимость между независимыми признаками? Как это влияет на качество уравнения линейной регрессии?
    - **Постройте попарные дигаммы рассеяния (seaborn.pairplot). Что показывает такая диаграмма? Для какого распределения целевого признака предсказания уравнения линейной регрессии будут точнее всего? Постройте для этого признака отдельную диаграмму рассеяния (seaborn.jointplot).**
    - Дополнительно используйте пакет plotly для построения двухмерной и трёхмерной диаграммы рассеяния. На диаграмме должен быть целевой признак, и два наиболее значимых независимых признака.
    - Бонус: дополнительно постройте диаграммы на свой выбор.
1. **Обучите модель линейной регрессии**
1. Разделите исходные данные на выборку для обучения (train) и отложенную выборку (test) для проверки результатов обучения. Оцените качество модели на этих выборках. Почему результаты могут отличаться?
    - **Вычисляйте ошибку MSE, MAE.  Используйте встроенные в sklearn функции.**
    - Вычислите коэффициент детерминации R2. О чём он говорит?
    - upd: приведите функцию потерь, пояснения к ней. Зачем нужна эта функция?
1. **Напишите пояснения и комментарии к коду. Поясняйте общий алгоритм, смысл действий**, понятия (std, mean, ...) **параметры вызываемых функций**, записанные в коде формулы (приведите их в LaTeX). Комментарии к коду можно оставлять в ячейках с кодом, остальные пояснения можно давать в ячейках с текстом.

[Короткая шпаргалка по линейной регрессии](https://github.com/ivtipm/ML/blob/main/slides%20etc/linreg/ML_intro_linreg.png?raw=true)

7. Выполните оценку модели методом Cross Validation (CV). Разбивайте все данные на 5 частей. Вычислите средний R2.\
Запишите: отличается ли R2 на тестовой выборке, котоую вы сделали ранее, от полученной по CV. Объясните это.

**Настройка VS Code для работы с jupyter notebook**

<details>

1. Скачайте и установить VS Code: https://code.visualstudio.com/
2. Установите дополнение Python от Microsoft.\
Оно поможет работать с виртуальными окружениями Питона.
2. Установите дополнение Jupyter от Microsoft\
Дальнейшие инструкции можно посмотреть во встроенной пошаговой инструкции: F1 > Welcome: Open Walkthrough... > Get started with jupyter notebook
3. Создайте файл с расширением .ipynb
4. Запустите ячейку с кодом: Shift + Enter. При необходимости установите рекомендуемые модули для работы с Jupyter Notebook и другие расширения для VS Code. Например Data Wrangler для удобного просмотра больших таблиц.

</details>

## Разведочный анализ данных (EDA). Классификация. Логистическая регрессия
**Цель работы** — изучить подход к решения реальных задач МО:
    - научиться использовать подход — построение базовых моделей (baseline)
    - улучшить навыки разведочного анализа данных и подготовки данных
    - научиться строить и анализировать вероятностные модели классификации

Решите задачу классификации. Используйте google colab, аналогичный сервис или jupyter notebook.

Целевая переменная приведена в последней колонке в файле `cleve.mod`. Значение `H` - пациент здоров, другие значения - пациент болен.

0. Приведите ссылку на задание, текст задания в коде (ipynb файле)
1. Загрузите датасет https://github.com/ivtipm/ML/blob/main/datasets/classification/heart_disease.zip
   Удобнее всего загружать так:
   ```
   pd.read_csv("cleve.mod", delim_whitespace=True, skiprows=20, header=None)
   # delim_whitespace=True столбцы разделены одним или несколькими пробелами
   # header=None - заголовок таблицы отсутствует
   # skiprows=20 - пропустить 20 строк, в которых содержится описание данных
   ```
   Описание датасета: https://archive.ics.uci.edu/dataset/45/heart+disease

   Или предложите свой датасет, сопоставимый по количеству и качеству данных с рекомендованным. Используйте свой датасет после согласования с преподавателем.
1. Реализуйте быстрое базовое решение (**baseline**):
    - сделайте целевой признак числовым
    - отбросьте все остальные нечисловые признаки
    - на полученном множестве признаков обучите модель логистической регрессии, оцените качество на тестовой выборке. Получилась ли модель удовлетворительного качества?
    - сохраните это решение. После выполнения второй части работы сравните качество финально модели с базовой.

1. Проведите **разведочный анализ данных**.
    - 7 point summary,
    - баланс классов,
    - Пропуски, дубликаты,
    - диаграммы рассеяния (pairplot), цветом показывайте значение целевого признака; можно ли на диаграммах провести разделяющую прямую между объектами разных классов?
    - диаграммы размаха, скрипичные диаграммы (violin plot),
    - матрица корреляции,
    - Интерактивные диаграммы на основе plotly.
    - Сделайте резюме по данным: опишите признаки, количество объектов, выбросы, пропуски, важные для предсказания признаки (Какие признаки сильнее всего влияют на целевой признак?). Какие распределения имеют признаки (нормальное, равномерное и т.п.)? Дополните резюме по желанию.
    - запишите несколько запросов к таблице с данными: простой запрос с фильтром, с составным условием, с отбором столбцов
    - выполните 2-3 группировки, в том числе группировки по нескольким критериям
1. **Подготовьте данные**:
    - удалите выбросы и дубликаты
    - Закодируйте категориальные признаки (используйте числовое и one-hot кодирование)
    - Масштабируйте признаки
    - Объясняйте действия, описывайте зачем их выполняете.
1. **Обучите модель** логистической регрессии. Приведите общую формулу, пояснения для неё.
1. **Оцените модель** на отложенной выборке (используйте `test_train_split`) и c помощью кросс-валидации.
Вычислите accuracy (общую точность), точность (precision), recall (полноту), меру f1. Опишите эти показатели.
Постройте матрицу ошибок. Используйте classification_report для получения метрик качества модели.
1. **Поэкспериментируйте** с процедурой обработки данных. Удаляйте лишние столбцы, модифицируйте колонки, удаляйте выбросы и т.п. Обучайте модель после каждого эксперимента. Записывайте как каждый этап влияет на качество модели.
1. Если признаки имеют примерно одинаковую шкалу (минимумы и максимумы признаков отличаются на несколько процентов, или стандартные отклонения и средние значения признаков отличаются на несколько процентов), то опишите **важность признаков**, на основе коэффициентов регрессии. Сравните это с важностью признаков, оцененной на основе коэффициента корреляции.
1. *Бонус: используйте Weight and Biases (wandb), MLflow и другие средства для **отслеживания экспериментов*** [github.com/ivtipm/ML/blob/main/plans/AI_intro_2025/2025_ML_intro.md](https://github.com/ivtipm/ML/blob/main/plans/AI_intro_2025/2025_ML_intro.md)
1. **Рефлексия**.
    - Какие шаги анализа и обработки наиболее сильно повлияли на качество модели?
    - Требуется ли ли перекрёстная проверка модели?
    - По желанию опишите свой опыт: с какими трудностями столкнулись и как их преодолели.


1. Создайте пайплайн для обработки данных. Добавьте в него модель. Сохраните пайплайн. Постройте веб-сервис на gradio и API сервер на основе полученного пайплайна. См. аналогичное задание по Языкам программирования для МО.
Интерфейсы сервисов должны позволять ввести значения признаков, получить вероятности классов.


**Справка**
Шпаргалка по Питону, анализу данных и МО: https://miro.com/app/board/uXjVNQC1rq8=/

Примеры:
- https://colab.research.google.com/drive/1AdbtsRkX0jRVByuAKJxchYPcciTgFpqh?usp=sharing#scrollTo=UmtvOZb6DzgM
- https://colab.research.google.com/drive/1kLfmJ4q81BNZtMs-oVX9cKJGsT3mYWmR#scrollTo=bG_ytuO0xECz

## Генетический алгоритм?


## Использование LLM?
*Задание в разработке*

1. Установите сервер для локального запуска LLM.
Примеры серверов:
- ollama - популярный сервер с CLI;
- llamacpp - более быстрый, но менее удобный аналог ollama;
- LM Studio
- Jan



*Добавьте возможность использовать внешний сервер с LLM, например c GigaChat*

2. Решите задачу используя LLM:
    - классификации текстов (Анализ тональности sentiment analysis, тематическая категоризация — topic classification, контент-модерация)
    - summarization, рецензирование?
    - Named Entity Recognition (NER) — распознавание сущностей
    - аннотирование изображения — image captioning (подпись к картинке)
    - классификация изображений, контент модерация
    - поиск аномалий?

## Тест



# Экзамен
