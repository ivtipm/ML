# pytorch


# tensor
tensor -- многомерный массив, основной тип данных pytorch.

**Перестановка размерностей**
```python
x = torch.tensor([[4, 2, 3],
                 [2, 6, 8]])
# установка размерностей в порядке: 1 и 0
torch.permute(x, (1,0) )

# результат
tensor([[4, 2],
        [2, 6],
        [3, 8]])
```

# Слои

## Embedding
`torch.nn.Embedding` создаёт эмбеддинги (вектора) слов.
https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html

Как правило используются вектора размерностью 100-300. 


```python
import torch
from torch import nn

# текст, где каждое слово представлено числом -- его номером в словаре (см. класс Vocab)
# нули (число-заполнитель -- padding index) поставлены для выравнивания всех тектов по одной длине 
text = torch.tensor( [[12, 27, 4, 56, 3, 81, 0, 0, 0, 0 ]] ) 		# [batch_size = 1, sequence_len = 10]


emb = nn.Embedding(num_embeddings = 100, 	# размер словаря
				   embedding_dim = 128, 	# размерность вектора
				   padding_idx = 0 			# 
				   )

emb( text )			# -> [batch_size, seq_len, emb_dim]

```

Результат:
```python
tensor([[[ 0.6943,  1.0508, -1.8730,  ..., -0.2826,  0.2395,  1.0945],
         [ 0.3233,  1.6904,  0.2099,  ...,  0.5310, -1.2767,  2.1296],
         [ 0.9116, -0.6055,  1.3053,  ..., -2.3760,  1.9805, -0.3670],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       grad_fn=<EmbeddingBackward0>)

```
